{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valthoraval/reinforcement-learning/blob/master/RL4_Tutorial_RLLIB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCS5xukfKEeJ"
      },
      "source": [
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://softwareengineeringdaily.com/wp-content/uploads/2020/02/ray-logo.png\" alt=\"drawing\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "# Why Ray ? \n",
        "As you all know, Python is an interpreted language: An interpreter executes the lines of code one by one. \n",
        "As a result, a python program takes much longer to execute than a binary file compiled with C or C++. \n",
        "Ok, but for a lot of reasons, we don't want to use C++ ...\n",
        "What's left : \n",
        "- **Optimize your code**: \n",
        "\n",
        "it seems obvious and yet we often forget it. Of course it has its limits ...\n",
        "\n",
        "- **Multithreading**: \n",
        "\n",
        "Distribute your code in several tasks that all have access to the same memory. However, if you know well python, you know the limits imposed by the GIL (Global Lock Interpreter) :\n",
        "<p align=\"center\">\n",
        "<img src=\"https://pbs.twimg.com/media/EZzAw78WAAE7d_D.jpg\" alt=\"drawing\" width=\"400\" />\n",
        "</p> \n",
        "\n",
        "\n",
        "- **Multiprocessing**: \n",
        "\n",
        "We're left with multiprocessing, but there are multiple ways to implement multiprocessing. You can use the python multiprocessing library. Also you will have to deal with the memory. In addition, when you want to scale efficiently on an HPC you will have to use a different framework like MPI. \n",
        "Also, You will probably have to rethink your machine learning application from the beginning.\n",
        "\n",
        "**The answer to your problems is Ray** : \n",
        "\n",
        "Ray offers an extremely simple api to distribute your code with minimal changes. In addition, Ray offers a whole bunch of libraries that allow machine learning applications while optimizing the use of resources to get the best models, as quickly as possible. \n",
        "Finally, one of the most interesting capabilities (to me) is that Ray scales very well on clusters.\n",
        "\n",
        "\n",
        "\n",
        "Today we will see the different librairies that ray offers but the focus will be on RLlib. \n",
        "**Part 1** of this tutorial presents very simply how ray core works. **The second** part gives an overview of the different machine learning libraries that ray offers and refers to introductory tutorials, the idea being just to draw your attention to the fact that these libraries exist.\n",
        "**The third** part is the heart of the tutorial, in this part we will see how to use RLlib a library of Ray that allows to do distributed RL.\n",
        "\n",
        "1. Introduction to Ray Core (25 min):\n",
        "  * init \n",
        "  * remote \n",
        "  * serialization\n",
        "  * ray on HPC (links)\n",
        "2. Ray librairies overview (10 min):\n",
        "  * Ray Data (links)\n",
        "  * Ray Train (links)\n",
        "  * Ray Tune (links)\n",
        "3. TP RLLIB (2h40): \n",
        "  * RLib overview \n",
        "  * First training : **CartPole-v0 with PPO** (1h)\n",
        "    * Hyper-parameters setting\n",
        "    * Ray Tune API\n",
        "    * Tensorboard or Weights and Biases : Framework to monitor your training\n",
        "    * Customize your training\n",
        "<p align=\"center\">\n",
        "<img src=\"https://bytepawn.com/images/cartpole.gif\" alt=\"drawing\" width=\"400\" />\n",
        "</p>\n",
        "  * Second training : **Pong with Rainbow** (1h40)\n",
        "    * Hyper-parameters setting\n",
        "    * Custom Environment\n",
        "    * Change your Model (Neural Network)\n",
        "    * Change the Loss (implementing Contrastive Loss)\n",
        "    * Quiz\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://blog.floydhub.com/content/images/2018/12/gif1.gif\" alt=\"drawing\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "  * Optional (Also i'm here to assist): \n",
        "    * Test other algorithms\n",
        "    * Create your own environment \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJnkR63eUryS"
      },
      "source": [
        "# Ray core\n",
        "Ray Core provides a small number of core primitives (i.e., tasks, actors, objects) for building and scaling distributed applications. Below we’ll walk through simple examples that show you how to turn your functions and classes easily into Ray tasks and actors, and how to work with Ray objects.\n",
        "\n",
        "See more : [Ray Core](https://docs.ray.io/en/latest/ray-core/walkthrough.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIqV3AsdWYXx"
      },
      "source": [
        "Fist let's install the library on your Colab sever :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50dbZdL4q7fy"
      },
      "outputs": [],
      "source": [
        "!pip install ray > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G8g2K8IPhDNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ba078a-3859-4717-9c58-b16f7b98112d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ray, version 2.2.0\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! ray --version "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9RlMJcp2hUWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a50b5b-1fd4-41ac-f208-632599b1c7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "print(pickle.format_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek_5N7YhWl79"
      },
      "source": [
        "## Init\n",
        "Ray provides a really simple API that allows you to initialize the module and use all its parallelization capabilities :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vrmTrgZ1qs-6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "72a32a26-6bf0-43e7-ea1a-e0a2ebafa125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 09:16:13,248\tINFO worker.py:1538 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RayContext(dashboard_url='', python_version='3.8.10', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '172.28.0.12', 'raylet_ip_address': '172.28.0.12', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-01-24_09-16-12_006575_402/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-01-24_09-16-12_006575_402/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-01-24_09-16-12_006575_402', 'metrics_export_port': 61217, 'gcs_address': '172.28.0.12:51844', 'address': '172.28.0.12:51844', 'dashboard_agent_listen_port': 52365, 'node_id': 'f6e766938975e0e02e0352e63b7efdd0bf35ab034cf22379e4a34c34'})"
            ],
            "text/html": [
              "<div>\n",
              "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
              "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
              "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
              "            <g id=\"layer-1\">\n",
              "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
              "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
              "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
              "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
              "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
              "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
              "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
              "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
              "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
              "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
              "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
              "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
              "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
              "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
              "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
              "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
              "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
              "            </g>\n",
              "        </svg>\n",
              "        <table>\n",
              "            <tr>\n",
              "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
              "                <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
              "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
              "            </tr>\n",
              "            \n",
              "        </table>\n",
              "    </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import ray \n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fh6mTVYY5v"
      },
      "source": [
        "Now that the ray server has been initialized, you can check your available ressources : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mw-a8p-XYYIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae439b65-cd0a-47e9-c41c-55120785cc99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accelerator_type:T4': 1.0,\n",
              " 'object_store_memory': 3983693414.0,\n",
              " 'GPU': 1.0,\n",
              " 'CPU': 2.0,\n",
              " 'memory': 7967386830.0,\n",
              " 'node:172.28.0.12': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ressources = ray.available_resources()\n",
        "ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWm6AH2LYvoK"
      },
      "source": [
        "* Unless you already changed your runtime type, the default one has no GPU. If you want a GPU : you can change the runtime type in the settings (Also you need to rerun everything)\n",
        "* Also you can call the ray init with multiple args such as **num_cpus** or **nump_gpus** to specify explicitly the ressources you will be using. Important when working on shared machines. \n",
        "See all the [parameters](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-init) of ray.init.\n",
        "* If you call twice ray.init in the notebook it will crash. You can call **ray.shutdown** if you want to ... shutdown the ray server ! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mawZyiHFZppS"
      },
      "source": [
        "## Remote\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSjRhCvdtUqG"
      },
      "source": [
        "Let's start with a simple example which make sense in an industrial context : let's create a function that counts up to n and check how long does it take to execute 2 times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "93f-zkm6bAVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58b0372-d10a-40fb-af3d-7f27d3edf664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It takes 52.28252863883972s to count 2 times up to 400000000 without ray\n"
          ]
        }
      ],
      "source": [
        "import time \n",
        "\n",
        "def usefull(n):\n",
        "  count=0\n",
        "  for _ in range(n):\n",
        "    count+=1\n",
        "  return count\n",
        "\n",
        "n=int(4e8)\n",
        "t0=time.time()\n",
        "[usefull(n) for k in range(2)]\n",
        "print(\"It takes {}s to count 2 times up to {} without ray\".format(time.time()-t0,n))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFfxZ4NTDq1J"
      },
      "source": [
        "Now with ray :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hERvaoyZDuj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b331a763-4b13-4191-d00c-9318ff7422c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It takes 35.22639608383179s to count 2 times up to 400000000 with ray\n"
          ]
        }
      ],
      "source": [
        "@ray.remote\n",
        "def usefull(n):\n",
        "  count=0\n",
        "  for _ in range(n):\n",
        "    count+=1\n",
        "  return count\n",
        "\n",
        "n=int(4e8)\n",
        "t0=time.time()\n",
        "ray.get([usefull.remote(n) for k in range(2)])\n",
        "print(\"It takes {}s to count 2 times up to {} with ray\".format(time.time()-t0,n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJTwiZR2EkBB"
      },
      "source": [
        "Ok it's faster. Unfortunately we can't see the clear advantages of using ray core because on Colab we have only 2 cores. But you get the idea.\n",
        "Also, it makes sense to use ray when the task at hand consumes more resources than the cost of setting up ray."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvc2w6wu2Ui6"
      },
      "source": [
        "## Serialization\n",
        "\n",
        "To better understand how Ray works you need to understand its [key concepts](https://docs.ray.io/en/latest/ray-core/key-concepts.html) \n",
        "Basically there are three concepts : \n",
        "\n",
        "* Tasks (which we just saw)\n",
        "* Actors (allowing to distribute classes and their methods)\n",
        "* Objects (allowing to distribute any object so that it can be called from any node)\n",
        "\n",
        "When ray is being used in a cluster mode, the informations needed to compute the task on the node are stored on the RAM of the node.\n",
        "Also some informations are not serializable making it impossible for ray to store.\n",
        "Let's see how to store object in the [Object store](https://docs.ray.io/en/releases-1.11.0/ray-core/memory-management.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VoQGG60J21zT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbe6f95-127a-4b28-f868-a9b857dc3020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obj ref :  ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000001000000)\n",
            "obj value :  1\n"
          ]
        }
      ],
      "source": [
        "y=1\n",
        "y_obj=ray.put(y)\n",
        "print('obj ref : ', y_obj)\n",
        "print('obj value : ', ray.get(y_obj))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrpuMSwh2qEw"
      },
      "source": [
        "Ray uses [cloudpickle](https://github.com/cloudpipe/cloudpickle) with which you can serialize **almost** anything. \n",
        "Now let's say you to want to use ray to distribute the training of your agent on a custom environment which call a specific framework. \n",
        "You won't be able to serialize the environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SHQCemdaLhYO",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "e96b67ff-bdce-451c-90ee-403eddc2d013"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c8c66039e06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mray_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(value, _owner)\u001b[0m\n\u001b[1;32m   2373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mprofiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ray.put\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2375\u001b[0;31m             \u001b[0mobject_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialize_owner_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mObjectStoreFullError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m             logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mput_object\u001b[0;34m(self, value, object_ref, owner_address)\u001b[0m\n\u001b[1;32m    609\u001b[0m             ), \"Local Mode does not support inserting with an ObjectRef\"\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mserialized_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_serialization_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;31m# This *must* be the first place that we construct this python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# ObjectRef because an entry with 0 local references is created when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mRawSerializedObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize_to_msgpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36m_serialize_to_msgpack\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpython_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_METADATA_TYPE_PYTHON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             pickle5_serialized_object = self._serialize_to_pickle5(\n\u001b[0m\u001b[1;32m    429\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36m_serialize_to_pickle5\u001b[0;34m(self, metadata, value)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_and_clear_contained_object_refs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_out_of_band_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36m_serialize_to_pickle5\u001b[0;34m(self, metadata, value)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_in_band_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             inband = pickle.dumps(\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'sqlite3.Connection' object"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "class Env:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.conn = sqlite3.connect(path) #this can't be serialized\n",
        "\n",
        "original = Env(\"/tmp/db\")\n",
        "ray_obj = ray.get(ray.put(original))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2iZIjbXMHkR"
      },
      "source": [
        "To enable the serialization, you need to provide ray with the serializable data needed to rebuilt the object on another node. This is done with __reduce__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AODt1DUUMeGT"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "class Env:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.conn = sqlite3.connect(path) #this can't be serialized\n",
        "\n",
        "    def __reduce__(self):\n",
        "        deserializer = Env\n",
        "        serialized_data = (self.path,)\n",
        "        return deserializer, serialized_data\n",
        "\n",
        "original = Env(\"/tmp/db\")\n",
        "ray_obj = ray.get(ray.put(original))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvMQ8eGuAaYx"
      },
      "source": [
        "usefull tips : \n",
        "\n",
        "* **Ray actor definition** : You only need to decorate the class with ray.remote\n",
        "* **ray.remote args** : you can specify the ressources used by each actors with the args of the decorator\n",
        "* **Object memory management** : In the ray.init or ray.remote you can specify the capacity (or the capacity used) of the object store memory to limit the RAM usage. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGe3hQiBZ1Tr"
      },
      "source": [
        "## Ray on HPC\n",
        "\n",
        "As you can see ray is really simple and it allows you distribute efficiently your code with no major changes. \n",
        "The most outstanding thing is that once you have designed your code to integrate Ray, there is almost nothing to do to run it on an HPC. So you can forget about abominations like MPI.\n",
        "Unfortunately we won't have time to run the example (This part is just to let you know that this feature exists).\n",
        "If you want to run basic example on Pando, here is all you need : \n",
        "\n",
        "Doc for running on [ AWS clursters ](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/index.html).\n",
        "\n",
        "Doc for running on [Slurm](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/slurm.html) (Pando)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q1F0YW7Mtsu"
      },
      "source": [
        "# Ray librairies overview\n",
        "\n",
        "Unfortunately we won't have time to discover in depth all the librairies.\n",
        "Here is a brief summary of the capabilities of these librairies\n",
        "\n",
        "## Ray Data\n",
        "\n",
        "Ray Data is a library for building distributed data pipelines with Ray. It provides a high-level interface for defining and executing data processing tasks, as well as tools for managing the lifecycle of those tasks. \n",
        "Ray Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for GPU batch inference. They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
        "\n",
        "Usefull link : [Processing NYC taxi data using Ray Datasets](https://docs.ray.io/en/latest/data/examples/nyc_taxi_basic_processing.html)\n",
        "\n",
        "## Ray Train \n",
        "Ray Train scales model training for popular ML frameworks such as Torch, XGBoost, TensorFlow, and more. It seamlessly integrates with other Ray librairies such as Tune:\n",
        "<p align=\"center\">\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/train-specific.svg\" alt=\"drawing\" width=\"600\" />\n",
        "</p> \n",
        "\n",
        "Here is the [Quick Start](https://docs.ray.io/en/latest/train/train.html#quick-start)\n",
        "\n",
        "## Ray Tune\n",
        "Ray Tune is a library that allows you to perform an optimal hyperparameter search for a given training. Indeed, not only this library allows to realize these evaluations in a distributed way but it also allows to improve this research with state-of-the-art methods such as bayesian optimization.\n",
        "Here is the [Quick start](https://docs.ray.io/en/latest/tune/getting-started.html#tune-tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Vt1ZFVltXI"
      },
      "source": [
        "# RLlib\n",
        "<p align=\"center\">\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/rllib-logo.png\" alt=\"drawing\" width=\"300\" />\n",
        "</p> \n",
        "\n",
        "## RLlib overview\n",
        "\n",
        "RLlib Algorithm classes coordinate the distributed workflow of running rollouts and optimizing policies. Algorithm classes leverage parallel iterators to implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of these patterns:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/a2c-arch.svg\" alt=\"drawing\" width=\"600\" />\n",
        "</p> \n",
        "\n",
        "As it can be seen on the figure, RLlib uses multiple RolloutWorkers which are actually ray core Actors in order to maximize the number of sample collected. \n",
        "Once the ReplayBuffer filled, the trainer sample batches and train the model (learner). Once the model updated, the new weights are sent to the Rollout Workers. And it goes on ...\n",
        "\n",
        "That's actually what's going on underneath, Also there are multiple API levels which allow you to customize the workflow. We will start with the high level APIs and finish with low levels.\n",
        "\n",
        "## CartPole with PPO\n",
        "Let's install everything so that we can use [gym](https://www.gymlibrary.dev/) properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9xNdyKC3uI5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750183c6-43bf-44ca-db32-297438cae79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 66.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-66.1.1\n",
            "Cloning into 'ray_course'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 9 (delta 2), reused 8 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), 3.36 KiB | 1.68 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!pip install gym[classic_control] > /dev/null 2>&1\n",
        "!git clone https://github.com/Paul-antoineLeTolguenec/ray_course.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pe1yF0O2W_-"
      },
      "source": [
        "### Environment \n",
        "We can't render the envrionment in colab so for each rollout we will record a video and watch it afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "wtp-QWVGxFFX",
        "outputId": "23b6d55f-a315-40e5-9eed-aa0b26038b9b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import pygame\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from ray_course.gym_recorder import *\n",
        "gymlogger.set_level(40) #error only\n",
        "display = Display(visible=0, size=(1400, 900)) #display\n",
        "display.start()\n",
        "\n",
        "env=wrap_env(gym.make('CartPole-v1')) #env\n",
        "\n",
        "# rollout\n",
        "s = env.reset()\n",
        "d=False\n",
        "while not d:\n",
        "    env.render('rgb_array')\n",
        "    a = env.action_space.sample()\n",
        "    s, r, d, i = env.step(a) \n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vxw4KJq2eoj"
      },
      "source": [
        "### Hyper-parameter setting\n",
        "\n",
        "Now we are going to use the PPO (Proximal Policy Optimization).\n",
        "Usefull links :     \n",
        "* [PPO article](https://arxiv.org/abs/1707.06347)\n",
        "* [PPO summary](https://paperswithcode.com/method/ppo#:~:text=Proximal%20Policy%20Optimization%2C%20or%20PPO,using%20only%20first%2Dorder%20optimization.) \n",
        "* [PPO definition in RLlib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ppo)\n",
        "Before going on that part it might be usefull to refresh your memory using the PPO summary.\n",
        "\n",
        "Now we are going to setup the configuration of the PPO algorithm using a simple dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MK3oqsT72kDN"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": 'CartPole-v1',\n",
        "\t\t# \"env_config\": ENV_CONFIG, #the env config is the dictionary that's pass to the environment when built\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "    \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "    # PPO config\n",
        "    \"gamma\": 0.95,\n",
        "    \"use_critic\": True,\n",
        "    \"use_gae\": True, #Generalized Advantage Estimate\n",
        "    \"lambda\": 1,\n",
        "    \"kl_coeff\": 0.2,\n",
        "    \"rollout_fragment_length\":1024, #number of steps in the environment for each Rollout Worker\n",
        "    \"train_batch_size\": 1024, \n",
        "    \"sgd_minibatch_size\": 64,\n",
        "    \"shuffle_sequences\": True, #Kind of experience replay for PPO\n",
        "    \"num_sgd_iter\": 16,\n",
        "    \"lr\": 1e-3,\n",
        "    \"lr_schedule\": None,\n",
        "    \"vf_loss_coeff\": 1.0,\n",
        "    \"model\": {\n",
        "        \"vf_share_layers\": False, \n",
        "    },\n",
        "    \"entropy_coeff\": 0.0,\n",
        "    \"entropy_coeff_schedule\": None,\n",
        "    \"clip_param\": 0.4,\n",
        "    \"vf_clip_param\": 10.0,\n",
        "    \"grad_clip\": None,\n",
        "    \"observation_filter\": \"NoFilter\"\n",
        "\t}\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2frmuM_dNA0D"
      },
      "source": [
        "Questions : \n",
        "\n",
        "* What's the clip_param ? \n",
        "* How does it affect the training ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tip7JRKSTXND"
      },
      "source": [
        "**Usefull tips** : There used to be a common conf dictionary where you could access all the variables of the configuration (but since the new version, i'm not able to access it anymore): \n",
        "* [Here](https://chuacheowhuan.github.io/RLlib_trainer_config/) is the file i'm talking about.\n",
        "* Alternatively you can import the Algorithm config from RLlib and plot it as a dict : example below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_6CALAgUTQ3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fba1e2-6d06-4656-e081-27452a2f632a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 09:19:10,128\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extra_python_environs_for_driver': {},\n",
              " 'extra_python_environs_for_worker': {},\n",
              " 'num_gpus': 0,\n",
              " 'num_cpus_per_worker': 1,\n",
              " 'num_gpus_per_worker': 0,\n",
              " '_fake_gpus': False,\n",
              " 'custom_resources_per_worker': {},\n",
              " 'placement_strategy': 'PACK',\n",
              " 'eager_tracing': False,\n",
              " 'eager_max_retraces': 20,\n",
              " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
              "  'inter_op_parallelism_threads': 2,\n",
              "  'gpu_options': {'allow_growth': True},\n",
              "  'log_device_placement': False,\n",
              "  'device_count': {'CPU': 1},\n",
              "  'allow_soft_placement': True},\n",
              " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
              "  'inter_op_parallelism_threads': 8},\n",
              " 'env': None,\n",
              " 'env_config': {},\n",
              " 'observation_space': None,\n",
              " 'action_space': None,\n",
              " 'env_task_fn': None,\n",
              " 'render_env': False,\n",
              " 'clip_rewards': None,\n",
              " 'normalize_actions': True,\n",
              " 'clip_actions': False,\n",
              " 'disable_env_checking': False,\n",
              " 'num_envs_per_worker': 1,\n",
              " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              " 'sample_async': False,\n",
              " 'enable_connectors': False,\n",
              " 'rollout_fragment_length': 'auto',\n",
              " 'batch_mode': 'truncate_episodes',\n",
              " 'remote_worker_envs': False,\n",
              " 'remote_env_batch_wait_ms': 0,\n",
              " 'validate_workers_after_construction': True,\n",
              " 'ignore_worker_failures': False,\n",
              " 'recreate_failed_workers': False,\n",
              " 'restart_failed_sub_environments': False,\n",
              " 'num_consecutive_worker_failures_tolerance': 100,\n",
              " 'horizon': None,\n",
              " 'soft_horizon': False,\n",
              " 'no_done_at_end': False,\n",
              " 'preprocessor_pref': 'deepmind',\n",
              " 'observation_filter': 'NoFilter',\n",
              " 'synchronize_filters': True,\n",
              " 'compress_observations': False,\n",
              " 'enable_tf1_exec_eagerly': False,\n",
              " 'sampler_perf_stats_ema_coef': None,\n",
              " 'gamma': 0.99,\n",
              " 'lr': 5e-05,\n",
              " 'train_batch_size': 4000,\n",
              " 'model': {'_use_default_native_models': False,\n",
              "  '_disable_preprocessor_api': False,\n",
              "  '_disable_action_flattening': False,\n",
              "  'fcnet_hiddens': [256, 256],\n",
              "  'fcnet_activation': 'tanh',\n",
              "  'conv_filters': None,\n",
              "  'conv_activation': 'relu',\n",
              "  'post_fcnet_hiddens': [],\n",
              "  'post_fcnet_activation': 'relu',\n",
              "  'free_log_std': False,\n",
              "  'no_final_linear': False,\n",
              "  'vf_share_layers': False,\n",
              "  'use_lstm': False,\n",
              "  'max_seq_len': 20,\n",
              "  'lstm_cell_size': 256,\n",
              "  'lstm_use_prev_action': False,\n",
              "  'lstm_use_prev_reward': False,\n",
              "  '_time_major': False,\n",
              "  'use_attention': False,\n",
              "  'attention_num_transformer_units': 1,\n",
              "  'attention_dim': 64,\n",
              "  'attention_num_heads': 1,\n",
              "  'attention_head_dim': 32,\n",
              "  'attention_memory_inference': 50,\n",
              "  'attention_memory_training': 50,\n",
              "  'attention_position_wise_mlp_dim': 32,\n",
              "  'attention_init_gru_gate_bias': 2.0,\n",
              "  'attention_use_n_prev_actions': 0,\n",
              "  'attention_use_n_prev_rewards': 0,\n",
              "  'framestack': True,\n",
              "  'dim': 84,\n",
              "  'grayscale': False,\n",
              "  'zero_mean': True,\n",
              "  'custom_model': None,\n",
              "  'custom_model_config': {},\n",
              "  'custom_action_dist': None,\n",
              "  'custom_preprocessor': None,\n",
              "  'lstm_use_prev_action_reward': -1},\n",
              " 'optimizer': {},\n",
              " 'max_requests_in_flight_per_sampler_worker': 2,\n",
              " 'explore': True,\n",
              " 'exploration_config': {'type': 'StochasticSampling'},\n",
              " 'input_config': {},\n",
              " 'actions_in_input_normalized': False,\n",
              " 'postprocess_inputs': False,\n",
              " 'shuffle_buffer_size': 0,\n",
              " 'output': None,\n",
              " 'output_config': {},\n",
              " 'output_compress_columns': ['obs', 'new_obs'],\n",
              " 'output_max_file_size': 67108864,\n",
              " 'offline_sampling': False,\n",
              " 'evaluation_interval': None,\n",
              " 'evaluation_duration': 10,\n",
              " 'evaluation_duration_unit': 'episodes',\n",
              " 'evaluation_sample_timeout_s': 180.0,\n",
              " 'evaluation_parallel_to_training': False,\n",
              " 'evaluation_config': None,\n",
              " 'off_policy_estimation_methods': {},\n",
              " 'ope_split_batch_by_episode': True,\n",
              " 'evaluation_num_workers': 0,\n",
              " 'always_attach_evaluation_results': False,\n",
              " 'enable_async_evaluation': False,\n",
              " 'in_evaluation': False,\n",
              " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
              " 'keep_per_episode_custom_metrics': False,\n",
              " 'metrics_episode_collection_timeout_s': 60.0,\n",
              " 'metrics_num_episodes_for_smoothing': 100,\n",
              " 'min_time_s_per_iteration': None,\n",
              " 'min_train_timesteps_per_iteration': 0,\n",
              " 'min_sample_timesteps_per_iteration': 0,\n",
              " 'export_native_model_files': False,\n",
              " 'logger_creator': None,\n",
              " 'logger_config': None,\n",
              " 'log_level': 'WARN',\n",
              " 'log_sys_usage': True,\n",
              " 'fake_sampler': False,\n",
              " 'seed': None,\n",
              " 'worker_cls': None,\n",
              " '_tf_policy_handles_more_than_one_loss': False,\n",
              " '_disable_preprocessor_api': False,\n",
              " '_disable_action_flattening': False,\n",
              " '_disable_execution_plan_api': True,\n",
              " 'simple_optimizer': -1,\n",
              " 'replay_sequence_length': None,\n",
              " 'lr_schedule': None,\n",
              " 'use_critic': True,\n",
              " 'use_gae': True,\n",
              " 'kl_coeff': 0.2,\n",
              " 'sgd_minibatch_size': 128,\n",
              " 'num_sgd_iter': 30,\n",
              " 'shuffle_sequences': True,\n",
              " 'vf_loss_coeff': 1.0,\n",
              " 'entropy_coeff': 0.0,\n",
              " 'entropy_coeff_schedule': None,\n",
              " 'clip_param': 0.3,\n",
              " 'vf_clip_param': 10.0,\n",
              " 'grad_clip': None,\n",
              " 'kl_target': 0.01,\n",
              " 'vf_share_layers': -1,\n",
              " 'lambda': 1.0,\n",
              " 'input': 'sampler',\n",
              " 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x7fc00b511790>},\n",
              "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.__init__.<locals>.<lambda>(aid, episode, worker, **kwargs)>,\n",
              "  'policies_to_train': None,\n",
              "  'policy_map_capacity': 100,\n",
              "  'policy_map_cache': None,\n",
              "  'count_steps_by': 'env_steps',\n",
              "  'observation_fn': None},\n",
              " 'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
              " 'create_env_on_driver': False,\n",
              " 'custom_eval_function': None,\n",
              " 'framework': 'tf',\n",
              " 'num_cpus_for_driver': 1,\n",
              " 'num_workers': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "algo = PPOConfig()\n",
        "algo.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXXOijwMTaI"
      },
      "source": [
        "Now that the config is defined, we are ready to train : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hmWqwYsOMF7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977daf6e-76bb-4ad5-ad6b-1de0757265ca"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-24 09:19:16,006\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "2023-01-24 09:19:17,450\tINFO algorithm_config.py:2492 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
            "2023-01-24 09:19:17,458\tINFO tensorboardx.py:42 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
            "2023-01-24 09:19:17,460\tWARNING unified.py:54 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
            "2023-01-24 09:19:17,465\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=3200)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=3200)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=3200)\u001b[0m 2023-01-24 09:19:21,603\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=3200)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=3200)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=3200)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=3200)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=3200)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=3200)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=3200)\u001b[0m 2023-01-24 09:19:22,183\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=3200)\u001b[0m 2023-01-24 09:19:22.789364: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-01-24 09:19:29,268\tINFO trainable.py:172 -- Trainable.setup took 11.805 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-01-24 09:19:29,274\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
            "2023-01-24 09:19:33,044\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  0\n",
            "epoch :  1\n",
            "epoch :  2\n",
            "epoch :  3\n",
            "epoch :  4\n",
            "epoch :  5\n",
            "epoch :  6\n",
            "epoch :  7\n",
            "epoch :  8\n",
            "epoch :  9\n",
            "epoch :  10\n",
            "epoch :  11\n",
            "epoch :  12\n",
            "epoch :  13\n",
            "epoch :  14\n",
            "epoch :  15\n",
            "epoch :  16\n",
            "epoch :  17\n",
            "epoch :  18\n",
            "epoch :  19\n",
            "epoch :  20\n",
            "epoch :  21\n",
            "epoch :  22\n",
            "epoch :  23\n",
            "epoch :  24\n",
            "epoch :  25\n",
            "epoch :  26\n",
            "epoch :  27\n",
            "epoch :  28\n",
            "epoch :  29\n"
          ]
        }
      ],
      "source": [
        "from ray.rllib.algorithms.ppo import PPO\n",
        "ray.shutdown() #shutdown before re-init\n",
        "ray.init() #re-init\n",
        "algo = PPO(config=CONFIG)\n",
        "for epoch in range(30):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxFA_T_0Mb1t"
      },
      "source": [
        "Once, you consider the training over, you can save the model so that it can be reused later : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pHwR_FALMZ3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffac7c6a-5e61-42f2-fd86-fd90e6d84596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved in directory /root/ray_results/PPO_CartPole-v1_2023-01-24_09-24-209k5lnb2c/checkpoint_000000\n"
          ]
        }
      ],
      "source": [
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNXKgwtnzI4x"
      },
      "source": [
        "Let's see how to re-instanciate the model you trained :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nYzpn4CFOnwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9504d5-64f4-4481-b937-a369fd1b6176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 09:32:45,484\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "\u001b[2m\u001b[36m(pid=7295)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=7295)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=7295)\u001b[0m 2023-01-24 09:32:53,834\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=7295)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=7295)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7295)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7295)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7295)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7295)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7295)\u001b[0m 2023-01-24 09:32:54,415\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7295)\u001b[0m 2023-01-24 09:32:54.422723: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-01-24 09:32:54,795\tINFO trainable.py:172 -- Trainable.setup took 11.663 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-01-24 09:32:54,802\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ],
      "source": [
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "algo = Algorithm.from_checkpoint(checkpoint_dir) #load the state of the algorithm where it was : Optimizer state, weights, ...\n",
        "policy=algo.get_policy() #get the policy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mqi17EtPFmx"
      },
      "source": [
        "Now, let's eval the model in the environment : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vE5AM7_jLF-0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "624b51c6-54ec-46a9-d94e-0c7f08c121b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pyvirtualdisplay import Display\n",
        "import gym\n",
        "# Eval\n",
        "display = Display(visible=0, size=(1400, 900)) #display\n",
        "display.start()\n",
        "env = gym.make('CartPole-v1')\n",
        "episode_reward = 0\n",
        "d = False\n",
        "s = env.reset()\n",
        "while not d:\n",
        "    env.render('rgb_array')\n",
        "    logits,_= policy.model({'obs': np.expand_dims(s,axis=0)})\n",
        "    a=np.argmax(logits)\n",
        "    s,r,d,i= env.step(a)\n",
        "    episode_reward += r\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adHOBXv7PUit"
      },
      "source": [
        "Note : As you can see, i used my own policy (argmax) on the logits. Alternatively, the policy has the method **compute_single_action**. However (for some really weird reasons) this method sample an action from the distribution instead of giving the optimal action. Also there is NO way (or maybe i didn't find it) to make it deterministic. (if you find a way please be kind and tell me)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yMg8aTjzRkM"
      },
      "source": [
        "### Ray Tune (RLlib API)\n",
        "When sing an algorithm to train your model, it might be usefull to test different combinations of hyper-parameters. The ray Tune API allow you this feature. The following example, show you how to lunch multiple training to test various learning rates with Tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fXnocDwv042a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28c78376-e870-4ab5-dd2e-f97baccc274c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 09:35:09,060\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "2023-01-24 09:35:10,650\tWARNING callback.py:108 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-01-24 09:39:06</td></tr>\n",
              "<tr><td>Running for: </td><td>00:03:55.39        </td></tr>\n",
              "<tr><td>Memory:      </td><td>3.6/12.7 GiB       </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:T4)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v1_65790_00000</td><td>TERMINATED</td><td>172.28.0.12:8246</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         69.9938</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  200.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">            200.98</td></tr>\n",
              "<tr><td>PPO_CartPole-v1_65790_00001</td><td>TERMINATED</td><td>172.28.0.12:8754</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         63.8652</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  214.29</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            214.29</td></tr>\n",
              "<tr><td>PPO_CartPole-v1_65790_00002</td><td>TERMINATED</td><td>172.28.0.12:9234</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         61.3223</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.19</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            200.19</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=8246)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=8246)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=8246)\u001b[0m 2023-01-24 09:35:15,317\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=8246)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=8246)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(PPO pid=8246)\u001b[0m 2023-01-24 09:35:15,892\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
            "\u001b[2m\u001b[36m(PPO pid=8246)\u001b[0m 2023-01-24 09:35:15,892\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(PPO pid=8246)\u001b[0m 2023-01-24 09:35:16,187\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=8313)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=8313)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=8313)\u001b[0m 2023-01-24 09:35:20,563\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=8313)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=8313)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8313)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8313)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8313)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8313)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8313)\u001b[0m 2023-01-24 09:35:21,137\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8313)\u001b[0m 2023-01-24 09:35:21.145437: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(PPO pid=8246)\u001b[0m 2023-01-24 09:35:22.213395: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(PPO pid=8246)\u001b[0m 2023-01-24 09:35:23,033\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname    </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip    </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                  </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v1_65790_00000</td><td style=\"text-align: right;\">                  32000</td><td>{&#x27;num_env_steps_sampled&#x27;: 32000, &#x27;num_env_steps_trained&#x27;: 32000, &#x27;num_agent_steps_sampled&#x27;: 32000, &#x27;num_agent_steps_trained&#x27;: 32000}</td><td>{}              </td><td>2023-01-24_09-36-33</td><td>True  </td><td style=\"text-align: right;\">            200.98</td><td>{}             </td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               200.98</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             387</td><td>b8a45a643c394eec8bdc78df64686294</td><td>7ee427987719</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.675000011920929, &#x27;cur_lr&#x27;: 0.009999999776482582, &#x27;total_loss&#x27;: 9.275783, &#x27;policy_loss&#x27;: -0.0063035875, &#x27;vf_loss&#x27;: 9.271451, &#x27;vf_explained_var&#x27;: 0.07053984, &#x27;kl&#x27;: 0.015754752, &#x27;entropy&#x27;: 0.50487226, &#x27;entropy_coeff&#x27;: 0.0, &#x27;model&#x27;: {}}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 6975.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 32000, &#x27;num_env_steps_trained&#x27;: 32000, &#x27;num_agent_steps_sampled&#x27;: 32000, &#x27;num_agent_steps_trained&#x27;: 32000}   </td><td style=\"text-align: right;\">                         8</td><td>172.28.0.12</td><td style=\"text-align: right;\">                    32000</td><td style=\"text-align: right;\">                    32000</td><td style=\"text-align: right;\">                  32000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  32000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 71.64166666666667, &#x27;ram_util_percent&#x27;: 28.2}              </td><td style=\"text-align: right;\"> 8246</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.295693917474337, &#x27;mean_inference_ms&#x27;: 0.6862552542622927, &#x27;mean_action_processing_ms&#x27;: 0.07213008781925276, &#x27;mean_env_wait_ms&#x27;: 0.06715550120090662, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 46.0, &#x27;episode_reward_mean&#x27;: 200.98, &#x27;episode_len_mean&#x27;: 200.98, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 12, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [137.0, 324.0, 162.0, 107.0, 141.0, 177.0, 102.0, 152.0, 115.0, 59.0, 136.0, 282.0, 110.0, 105.0, 90.0, 110.0, 122.0, 109.0, 109.0, 130.0, 49.0, 197.0, 307.0, 121.0, 64.0, 144.0, 154.0, 63.0, 46.0, 158.0, 168.0, 99.0, 239.0, 113.0, 316.0, 96.0, 162.0, 192.0, 90.0, 138.0, 68.0, 302.0, 136.0, 144.0, 180.0, 237.0, 72.0, 136.0, 105.0, 121.0, 364.0, 240.0, 98.0, 110.0, 383.0, 235.0, 145.0, 500.0, 383.0, 169.0, 500.0, 206.0, 144.0, 145.0, 162.0, 182.0, 283.0, 276.0, 348.0, 81.0, 379.0, 235.0, 460.0, 106.0, 193.0, 126.0, 228.0, 192.0, 108.0, 189.0, 55.0, 205.0, 244.0, 143.0, 236.0, 255.0, 291.0, 228.0, 500.0, 464.0, 343.0, 218.0, 277.0, 174.0, 445.0, 347.0, 500.0, 97.0, 500.0, 210.0], &#x27;episode_lengths&#x27;: [137, 324, 162, 107, 141, 177, 102, 152, 115, 59, 136, 282, 110, 105, 90, 110, 122, 109, 109, 130, 49, 197, 307, 121, 64, 144, 154, 63, 46, 158, 168, 99, 239, 113, 316, 96, 162, 192, 90, 138, 68, 302, 136, 144, 180, 237, 72, 136, 105, 121, 364, 240, 98, 110, 383, 235, 145, 500, 383, 169, 500, 206, 144, 145, 162, 182, 283, 276, 348, 81, 379, 235, 460, 106, 193, 126, 228, 192, 108, 189, 55, 205, 244, 143, 236, 255, 291, 228, 500, 464, 343, 218, 277, 174, 445, 347, 500, 97, 500, 210]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.295693917474337, &#x27;mean_inference_ms&#x27;: 0.6862552542622927, &#x27;mean_action_processing_ms&#x27;: 0.07213008781925276, &#x27;mean_env_wait_ms&#x27;: 0.06715550120090662, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             69.9938</td><td style=\"text-align: right;\">           8.57892</td><td style=\"text-align: right;\">       69.9938</td><td>{&#x27;training_iteration_time_ms&#x27;: 8744.258, &#x27;load_time_ms&#x27;: 0.342, &#x27;load_throughput&#x27;: 11682281.138, &#x27;learn_time_ms&#x27;: 4219.895, &#x27;learn_throughput&#x27;: 947.891, &#x27;synch_weights_time_ms&#x27;: 3.266}</td><td style=\"text-align: right;\"> 1674552993</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            32000</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">65790_00000</td><td style=\"text-align: right;\">      6.85302</td></tr>\n",
              "<tr><td>PPO_CartPole-v1_65790_00001</td><td style=\"text-align: right;\">                  28000</td><td>{&#x27;num_env_steps_sampled&#x27;: 28000, &#x27;num_env_steps_trained&#x27;: 28000, &#x27;num_agent_steps_sampled&#x27;: 28000, &#x27;num_agent_steps_trained&#x27;: 28000}</td><td>{}              </td><td>2023-01-24_09-37-50</td><td>True  </td><td style=\"text-align: right;\">            214.29</td><td>{}             </td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               214.29</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             311</td><td>9b06afa00bb94739a08bea7bacf512fa</td><td>7ee427987719</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.30000001192092896, &#x27;cur_lr&#x27;: 0.0010000000474974513, &#x27;total_loss&#x27;: 9.655699, &#x27;policy_loss&#x27;: -0.021651858, &#x27;vf_loss&#x27;: 9.675178, &#x27;vf_explained_var&#x27;: -0.498937, &#x27;kl&#x27;: 0.007243863, &#x27;entropy&#x27;: 0.5249922, &#x27;entropy_coeff&#x27;: 0.0, &#x27;model&#x27;: {}}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 6045.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 28000, &#x27;num_env_steps_trained&#x27;: 28000, &#x27;num_agent_steps_sampled&#x27;: 28000, &#x27;num_agent_steps_trained&#x27;: 28000}   </td><td style=\"text-align: right;\">                         7</td><td>172.28.0.12</td><td style=\"text-align: right;\">                    28000</td><td style=\"text-align: right;\">                    28000</td><td style=\"text-align: right;\">                  28000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  28000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 71.85, &#x27;ram_util_percent&#x27;: 28.2}                          </td><td style=\"text-align: right;\"> 8754</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.3060228009427956, &#x27;mean_inference_ms&#x27;: 0.7212815535882483, &#x27;mean_action_processing_ms&#x27;: 0.07828267459150995, &#x27;mean_env_wait_ms&#x27;: 0.06871937105660937, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 11.0, &#x27;episode_reward_mean&#x27;: 214.29, &#x27;episode_len_mean&#x27;: 214.29, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 10, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [75.0, 102.0, 25.0, 84.0, 31.0, 41.0, 175.0, 189.0, 31.0, 24.0, 63.0, 112.0, 16.0, 82.0, 83.0, 77.0, 17.0, 11.0, 21.0, 26.0, 125.0, 51.0, 53.0, 131.0, 55.0, 52.0, 223.0, 217.0, 123.0, 245.0, 364.0, 300.0, 49.0, 28.0, 138.0, 294.0, 55.0, 167.0, 103.0, 95.0, 151.0, 182.0, 206.0, 71.0, 54.0, 202.0, 140.0, 12.0, 281.0, 94.0, 182.0, 216.0, 177.0, 217.0, 277.0, 347.0, 129.0, 254.0, 500.0, 229.0, 320.0, 268.0, 201.0, 281.0, 35.0, 122.0, 362.0, 375.0, 231.0, 178.0, 251.0, 500.0, 247.0, 307.0, 316.0, 170.0, 308.0, 207.0, 344.0, 500.0, 330.0, 195.0, 500.0, 500.0, 64.0, 500.0, 413.0, 500.0, 427.0, 500.0, 496.0, 500.0, 500.0, 167.0, 500.0, 467.0, 144.0, 173.0, 500.0, 456.0], &#x27;episode_lengths&#x27;: [75, 102, 25, 84, 31, 41, 175, 189, 31, 24, 63, 112, 16, 82, 83, 77, 17, 11, 21, 26, 125, 51, 53, 131, 55, 52, 223, 217, 123, 245, 364, 300, 49, 28, 138, 294, 55, 167, 103, 95, 151, 182, 206, 71, 54, 202, 140, 12, 281, 94, 182, 216, 177, 217, 277, 347, 129, 254, 500, 229, 320, 268, 201, 281, 35, 122, 362, 375, 231, 178, 251, 500, 247, 307, 316, 170, 308, 207, 344, 500, 330, 195, 500, 500, 64, 500, 413, 500, 427, 500, 496, 500, 500, 167, 500, 467, 144, 173, 500, 456]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.3060228009427956, &#x27;mean_inference_ms&#x27;: 0.7212815535882483, &#x27;mean_action_processing_ms&#x27;: 0.07828267459150995, &#x27;mean_env_wait_ms&#x27;: 0.06871937105660937, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}                             </td><td style=\"text-align: right;\">             63.8652</td><td style=\"text-align: right;\">           8.59494</td><td style=\"text-align: right;\">       63.8652</td><td>{&#x27;training_iteration_time_ms&#x27;: 9117.874, &#x27;load_time_ms&#x27;: 0.296, &#x27;load_throughput&#x27;: 13498909.425, &#x27;learn_time_ms&#x27;: 4343.96, &#x27;learn_throughput&#x27;: 920.819, &#x27;synch_weights_time_ms&#x27;: 3.354} </td><td style=\"text-align: right;\"> 1674553070</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            28000</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">65790_00001</td><td style=\"text-align: right;\">      6.7764 </td></tr>\n",
              "<tr><td>PPO_CartPole-v1_65790_00002</td><td style=\"text-align: right;\">                  28000</td><td>{&#x27;num_env_steps_sampled&#x27;: 28000, &#x27;num_env_steps_trained&#x27;: 28000, &#x27;num_agent_steps_sampled&#x27;: 28000, &#x27;num_agent_steps_trained&#x27;: 28000}</td><td>{}              </td><td>2023-01-24_09-39-05</td><td>True  </td><td style=\"text-align: right;\">            200.19</td><td>{}             </td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               200.19</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">             345</td><td>04e0e4c3b2ac45abb88914e7dd2ec1b0</td><td>7ee427987719</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.15000000596046448, &#x27;cur_lr&#x27;: 9.999999747378752e-05, &#x27;total_loss&#x27;: 9.833417, &#x27;policy_loss&#x27;: -0.018744165, &#x27;vf_loss&#x27;: 9.851768, &#x27;vf_explained_var&#x27;: -0.2636076, &#x27;kl&#x27;: 0.0026369807, &#x27;entropy&#x27;: 0.55642956, &#x27;entropy_coeff&#x27;: 0.0, &#x27;model&#x27;: {}}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 6045.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 28000, &#x27;num_env_steps_trained&#x27;: 28000, &#x27;num_agent_steps_sampled&#x27;: 28000, &#x27;num_agent_steps_trained&#x27;: 28000}</td><td style=\"text-align: right;\">                         7</td><td>172.28.0.12</td><td style=\"text-align: right;\">                    28000</td><td style=\"text-align: right;\">                    28000</td><td style=\"text-align: right;\">                  28000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  28000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 72.33846153846153, &#x27;ram_util_percent&#x27;: 28.199999999999996}</td><td style=\"text-align: right;\"> 9234</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.2940719525174704, &#x27;mean_inference_ms&#x27;: 0.6801228316689552, &#x27;mean_action_processing_ms&#x27;: 0.07634789812660502, &#x27;mean_env_wait_ms&#x27;: 0.06958441575906199, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 500.0, &#x27;episode_reward_min&#x27;: 11.0, &#x27;episode_reward_mean&#x27;: 200.19, &#x27;episode_len_mean&#x27;: 200.19, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 8, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [39.0, 41.0, 15.0, 29.0, 44.0, 48.0, 12.0, 57.0, 116.0, 24.0, 74.0, 236.0, 252.0, 107.0, 43.0, 38.0, 265.0, 22.0, 234.0, 106.0, 117.0, 41.0, 23.0, 156.0, 35.0, 96.0, 181.0, 66.0, 11.0, 116.0, 174.0, 113.0, 24.0, 124.0, 137.0, 103.0, 197.0, 52.0, 47.0, 112.0, 248.0, 110.0, 82.0, 21.0, 158.0, 116.0, 131.0, 121.0, 198.0, 287.0, 201.0, 221.0, 300.0, 154.0, 234.0, 138.0, 101.0, 198.0, 129.0, 187.0, 401.0, 208.0, 161.0, 12.0, 176.0, 153.0, 185.0, 183.0, 269.0, 277.0, 281.0, 396.0, 252.0, 500.0, 209.0, 239.0, 226.0, 428.0, 150.0, 404.0, 443.0, 403.0, 257.0, 301.0, 359.0, 430.0, 311.0, 348.0, 320.0, 358.0, 285.0, 411.0, 500.0, 500.0, 274.0, 500.0, 500.0, 500.0, 452.0, 395.0], &#x27;episode_lengths&#x27;: [39, 41, 15, 29, 44, 48, 12, 57, 116, 24, 74, 236, 252, 107, 43, 38, 265, 22, 234, 106, 117, 41, 23, 156, 35, 96, 181, 66, 11, 116, 174, 113, 24, 124, 137, 103, 197, 52, 47, 112, 248, 110, 82, 21, 158, 116, 131, 121, 198, 287, 201, 221, 300, 154, 234, 138, 101, 198, 129, 187, 401, 208, 161, 12, 176, 153, 185, 183, 269, 277, 281, 396, 252, 500, 209, 239, 226, 428, 150, 404, 443, 403, 257, 301, 359, 430, 311, 348, 320, 358, 285, 411, 500, 500, 274, 500, 500, 500, 452, 395]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.2940719525174704, &#x27;mean_inference_ms&#x27;: 0.6801228316689552, &#x27;mean_action_processing_ms&#x27;: 0.07634789812660502, &#x27;mean_env_wait_ms&#x27;: 0.06958441575906199, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}                    </td><td style=\"text-align: right;\">             61.3223</td><td style=\"text-align: right;\">           8.70303</td><td style=\"text-align: right;\">       61.3223</td><td>{&#x27;training_iteration_time_ms&#x27;: 8755.377, &#x27;load_time_ms&#x27;: 0.41, &#x27;load_throughput&#x27;: 9745291.843, &#x27;learn_time_ms&#x27;: 4245.303, &#x27;learn_throughput&#x27;: 942.218, &#x27;synch_weights_time_ms&#x27;: 3.346}  </td><td style=\"text-align: right;\"> 1674553145</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            28000</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">65790_00002</td><td style=\"text-align: right;\">      6.9553 </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=8754)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=8754)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=8754)\u001b[0m 2023-01-24 09:36:38,688\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=8754)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=8754)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(PPO pid=8754)\u001b[0m 2023-01-24 09:36:39,288\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
            "\u001b[2m\u001b[36m(PPO pid=8754)\u001b[0m 2023-01-24 09:36:39,288\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(PPO pid=8754)\u001b[0m 2023-01-24 09:36:39,590\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=8819)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=8819)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=8819)\u001b[0m 2023-01-24 09:36:43,990\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=8819)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=8819)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8819)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8819)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8819)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8819)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8819)\u001b[0m 2023-01-24 09:36:44,554\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=8819)\u001b[0m 2023-01-24 09:36:44.562347: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(PPO pid=8754)\u001b[0m 2023-01-24 09:36:45.522231: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(PPO pid=8754)\u001b[0m 2023-01-24 09:36:46,352\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
            "\u001b[2m\u001b[36m(pid=9234)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=9234)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=9234)\u001b[0m 2023-01-24 09:37:56,679\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=9234)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=9234)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(PPO pid=9234)\u001b[0m 2023-01-24 09:37:57,287\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
            "\u001b[2m\u001b[36m(PPO pid=9234)\u001b[0m 2023-01-24 09:37:57,287\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(PPO pid=9234)\u001b[0m 2023-01-24 09:37:57,596\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=9297)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=9297)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=9297)\u001b[0m 2023-01-24 09:38:02,096\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=9297)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=9297)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=9297)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=9297)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=9297)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=9297)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=9297)\u001b[0m 2023-01-24 09:38:02,690\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=9297)\u001b[0m 2023-01-24 09:38:02.697568: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(PPO pid=9234)\u001b[0m 2023-01-24 09:38:03.700708: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(PPO pid=9234)\u001b[0m 2023-01-24 09:38:04,536\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
            "2023-01-24 09:39:06,566\tINFO tune.py:762 -- Total run time: 235.92 seconds (235.37 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray import air, tune\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "config = PPOConfig().training(lr= tune.grid_search([0.01, 0.001, 0.0001])).rollouts(num_rollout_workers=1).resources(num_gpus=0).environment(env=\"CartPole-v1\")\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    \"PPO\",\n",
        "    run_config=air.RunConfig(\n",
        "        stop={\"episode_reward_mean\": 200},\n",
        "        local_dir=\"./results\", \n",
        "        name=\"PPO\"\n",
        "    ),\n",
        "    param_space=config,\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "# Get the best result based on a particular metric.\n",
        "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
        "\n",
        "# Get the best checkpoint corresponding to the best result.\n",
        "best_checkpoint = best_result.checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqW1y7zg2rkG"
      },
      "source": [
        "### TensorBoard or Weights and Biaises (Monitoring your algorithm)\n",
        "\n",
        "We just saw how to use the basic API of RLlib. However we didn't have much data during the training (which can be nice for monitoring). \n",
        "These informations are essential because they can allow you, for example, to stop a training when assymptotically we see that there is no learning.\n",
        "From now you have multiple solutions :\n",
        "\n",
        "* **Tensorboard** :\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.tensorflow.org/static/site-assets/images/project-logos/tensorboard-logo-social.png\" alt=\"drawing\" width=\"400\" />\n",
        "</p> \n",
        "\n",
        "TensorBoard stores the data localy on logfile and you can access the interface in your browser on the port you specified.\n",
        "It's well suited for RLlib because the library generates its own logfile\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gzufCK2_AI27"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorboardx > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hdpghdm8_f7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c7144b-9fc2-46df-ff01-5f680d6271dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ],
      "source": [
        "%tensorboard --logdir /content/results/PPO/NAME_OF_YOUR_EXPERIMENT_FOLDER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-JyBClpUW8Q"
      },
      "source": [
        "* **Weights and Biases** :\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://assets.website-files.com/5ac6b7f2924c656f2b13a88c/6077a58f02c7ef0e37fde627_weights%20and%20biases%20workspace.jpg\" alt=\"drawing\" width=\"400\" />\n",
        "\n",
        "Weights and Biases is a great tool to visualize data in real time (or near real time). \n",
        "The advantage over tensorboard is that the data is stored in the cloud.\n",
        "Steps to use weights and biases : \n",
        "1. Create an account : [here](https://wandb.ai/site)\n",
        "2.Copy your API key : [here](https://wandb.ai/home)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0M7KehmLdJ4U"
      },
      "outputs": [],
      "source": [
        "!pip install wandb > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dLTzdd1dsps",
        "outputId": "7e4f50a6-c02a-4cce-c40a-75531fbb5691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "! wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HVfKXpNrCqN"
      },
      "source": [
        "Basic usage of wandb: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4V0AqpnSkQ4-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "eeed8a4c-99a5-4437-ec7b-3c8b3c1c683d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvalentin-thoraval\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230124_094715-jzfoyqop</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/valentin-thoraval/Test/runs/jzfoyqop\" target=\"_blank\">fortuitous-monkey-1</a></strong> to <a href=\"https://wandb.ai/valentin-thoraval/Test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/valentin-thoraval/Test\" target=\"_blank\">https://wandb.ai/valentin-thoraval/Test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/valentin-thoraval/Test/runs/jzfoyqop\" target=\"_blank\">https://wandb.ai/valentin-thoraval/Test/runs/jzfoyqop</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(project='Test')\n",
        "wandb.run.name='run'\n",
        "for k in range(50):\n",
        "    wandb.log({'data': k})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB_WRr719UY8"
      },
      "source": [
        "To use wandb with Ray, it is necessary to implement a specific CallBack( functions that are called to produce the log) that call wandb.\n",
        "[Here](https://docs.ray.io/en/latest/_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks) is the DefaultCallback. You can overide functions so that you output your own metrics. I propose an implementation that uses wandb in the ray_course.custom_callbacks. Feel free to modify it. In the below example we see how to specify the CustomCallback : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IH6eZTT4fC_c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "457636f8-11bd-414a-e5c2-18416ec84a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 09:48:04,765\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "2023-01-24 09:48:06,221\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property reuse_actors not supported.\n",
            "\u001b[2m\u001b[36m(pid=12428)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=12428)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=12428)\u001b[0m 2023-01-24 09:48:10,423\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=12428)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=12428)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=12428)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=12428)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=12428)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=12428)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=12428)\u001b[0m 2023-01-24 09:48:11,574\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=12428)\u001b[0m 2023-01-24 09:48:11.582412: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:jzfoyqop) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data</td><td>49</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fortuitous-monkey-1</strong> at: <a href=\"https://wandb.ai/valentin-thoraval/Test/runs/jzfoyqop\" target=\"_blank\">https://wandb.ai/valentin-thoraval/Test/runs/jzfoyqop</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230124_094715-jzfoyqop/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:jzfoyqop). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230124_094811-kwwolpr4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/valentin-thoraval/RLlib/runs/kwwolpr4\" target=\"_blank\">golden-fireworks-1</a></strong> to <a href=\"https://wandb.ai/valentin-thoraval/RLlib\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/valentin-thoraval/RLlib\" target=\"_blank\">https://wandb.ai/valentin-thoraval/RLlib</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/valentin-thoraval/RLlib/runs/kwwolpr4\" target=\"_blank\">https://wandb.ai/valentin-thoraval/RLlib/runs/kwwolpr4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 09:48:16,244\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agent_timesteps_total: 1024\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1024\n",
            "  num_agent_steps_trained: 1024\n",
            "  num_env_steps_sampled: 1024\n",
            "  num_env_steps_trained: 1024\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-48-26\n",
            "done: false\n",
            "episode_len_mean: 21.3125\n",
            "episode_media: {}\n",
            "episode_reward_max: 75.0\n",
            "episode_reward_mean: 21.3125\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 48\n",
            "episodes_total: 48\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.6395122408866882\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.05597608909010887\n",
            "        policy_loss: -0.0668821930885315\n",
            "        total_loss: 6.033782005310059\n",
            "        vf_explained_var: 0.17998509109020233\n",
            "        vf_loss: 6.089468955993652\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 128.5\n",
            "  num_agent_steps_sampled: 1024\n",
            "  num_agent_steps_trained: 1024\n",
            "  num_env_steps_sampled: 1024\n",
            "  num_env_steps_trained: 1024\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1024\n",
            "num_agent_steps_trained: 1024\n",
            "num_env_steps_sampled: 1024\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 1024\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 53.592857142857135\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.05478905468452267\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06475309046303356\n",
            "  mean_inference_ms: 3.29407901298709\n",
            "  mean_raw_obs_processing_ms: 0.3128824001405297\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 21.3125\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 75.0\n",
            "  episode_reward_mean: 21.3125\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 48\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.05478905468452267\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06475309046303356\n",
            "    mean_inference_ms: 3.29407901298709\n",
            "    mean_raw_obs_processing_ms: 0.3128824001405297\n",
            "time_since_restore: 9.806777954101562\n",
            "time_this_iter_s: 9.806777954101562\n",
            "time_total_s: 9.806777954101562\n",
            "timers:\n",
            "  learn_throughput: 171.775\n",
            "  learn_time_ms: 5961.272\n",
            "  synch_weights_time_ms: 5.717\n",
            "  training_iteration_time_ms: 9800.593\n",
            "timestamp: 1674553706\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1024\n",
            "training_iteration: 1\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 2048\n",
            "counters:\n",
            "  num_agent_steps_sampled: 2048\n",
            "  num_agent_steps_trained: 2048\n",
            "  num_env_steps_sampled: 2048\n",
            "  num_env_steps_trained: 2048\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-48-35\n",
            "done: false\n",
            "episode_len_mean: 35.275862068965516\n",
            "episode_media: {}\n",
            "episode_reward_max: 172.0\n",
            "episode_reward_mean: 35.275862068965516\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 10\n",
            "episodes_total: 58\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.5633053779602051\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.028706859797239304\n",
            "        policy_loss: -0.038741353899240494\n",
            "        total_loss: 5.1944451332092285\n",
            "        vf_explained_var: 0.2744229733943939\n",
            "        vf_loss: 5.224574089050293\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 384.5\n",
            "  num_agent_steps_sampled: 2048\n",
            "  num_agent_steps_trained: 2048\n",
            "  num_env_steps_sampled: 2048\n",
            "  num_env_steps_trained: 2048\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 2048\n",
            "num_agent_steps_trained: 2048\n",
            "num_env_steps_sampled: 2048\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 2048\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 55.385714285714286\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.054625627195598714\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.0647318368625959\n",
            "  mean_inference_ms: 3.290429074727492\n",
            "  mean_raw_obs_processing_ms: 0.3102533062539492\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 35.275862068965516\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 172.0\n",
            "  episode_reward_mean: 35.275862068965516\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 10\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.054625627195598714\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0647318368625959\n",
            "    mean_inference_ms: 3.290429074727492\n",
            "    mean_raw_obs_processing_ms: 0.3102533062539492\n",
            "time_since_restore: 19.48115301132202\n",
            "time_this_iter_s: 9.674375057220459\n",
            "time_total_s: 19.48115301132202\n",
            "timers:\n",
            "  learn_throughput: 172.479\n",
            "  learn_time_ms: 5936.965\n",
            "  synch_weights_time_ms: 5.481\n",
            "  training_iteration_time_ms: 9734.835\n",
            "timestamp: 1674553715\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2048\n",
            "training_iteration: 2\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 3072\n",
            "counters:\n",
            "  num_agent_steps_sampled: 3072\n",
            "  num_agent_steps_trained: 3072\n",
            "  num_env_steps_sampled: 3072\n",
            "  num_env_steps_trained: 3072\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-48-45\n",
            "done: false\n",
            "episode_len_mean: 47.016129032258064\n",
            "episode_media: {}\n",
            "episode_reward_max: 295.0\n",
            "episode_reward_mean: 47.016129032258064\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 62\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.5731902718544006\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009574340656399727\n",
            "        policy_loss: -0.006110223941504955\n",
            "        total_loss: 1.4286103248596191\n",
            "        vf_explained_var: -0.4828314483165741\n",
            "        vf_loss: 1.4304120540618896\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 640.5\n",
            "  num_agent_steps_sampled: 3072\n",
            "  num_agent_steps_trained: 3072\n",
            "  num_env_steps_sampled: 3072\n",
            "  num_env_steps_trained: 3072\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 3072\n",
            "num_agent_steps_trained: 3072\n",
            "num_env_steps_sampled: 3072\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 3072\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 54.27857142857142\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.0545612168459977\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06473255829256\n",
            "  mean_inference_ms: 3.2894046686710574\n",
            "  mean_raw_obs_processing_ms: 0.3089753983461651\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 47.016129032258064\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 295.0\n",
            "  episode_reward_mean: 47.016129032258064\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0545612168459977\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06473255829256\n",
            "    mean_inference_ms: 3.2894046686710574\n",
            "    mean_raw_obs_processing_ms: 0.3089753983461651\n",
            "time_since_restore: 29.219728469848633\n",
            "time_this_iter_s: 9.738575458526611\n",
            "time_total_s: 29.219728469848633\n",
            "timers:\n",
            "  learn_throughput: 172.299\n",
            "  learn_time_ms: 5943.174\n",
            "  synch_weights_time_ms: 5.71\n",
            "  training_iteration_time_ms: 9734.067\n",
            "timestamp: 1674553725\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3072\n",
            "training_iteration: 3\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 4096\n",
            "counters:\n",
            "  num_agent_steps_sampled: 4096\n",
            "  num_agent_steps_trained: 4096\n",
            "  num_env_steps_sampled: 4096\n",
            "  num_env_steps_trained: 4096\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-48-55\n",
            "done: false\n",
            "episode_len_mean: 58.3235294117647\n",
            "episode_media: {}\n",
            "episode_reward_max: 295.0\n",
            "episode_reward_mean: 58.3235294117647\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 6\n",
            "episodes_total: 68\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.5227793455123901\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013075173832476139\n",
            "        policy_loss: -0.020551614463329315\n",
            "        total_loss: 1.5290700197219849\n",
            "        vf_explained_var: 0.5435967445373535\n",
            "        vf_loss: 1.5437376499176025\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 896.5\n",
            "  num_agent_steps_sampled: 4096\n",
            "  num_agent_steps_trained: 4096\n",
            "  num_env_steps_sampled: 4096\n",
            "  num_env_steps_trained: 4096\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 4096\n",
            "num_agent_steps_trained: 4096\n",
            "num_env_steps_sampled: 4096\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 4096\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 56.32857142857142\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.054477780710659916\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.064729865691187\n",
            "  mean_inference_ms: 3.2880036784059423\n",
            "  mean_raw_obs_processing_ms: 0.30695997662390867\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 58.3235294117647\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 295.0\n",
            "  episode_reward_mean: 58.3235294117647\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 6\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.054477780710659916\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.064729865691187\n",
            "    mean_inference_ms: 3.2880036784059423\n",
            "    mean_raw_obs_processing_ms: 0.30695997662390867\n",
            "time_since_restore: 38.92647123336792\n",
            "time_this_iter_s: 9.706742763519287\n",
            "time_total_s: 38.92647123336792\n",
            "timers:\n",
            "  learn_throughput: 172.398\n",
            "  learn_time_ms: 5939.749\n",
            "  synch_weights_time_ms: 5.552\n",
            "  training_iteration_time_ms: 9725.887\n",
            "timestamp: 1674553735\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4096\n",
            "training_iteration: 4\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 5120\n",
            "counters:\n",
            "  num_agent_steps_sampled: 5120\n",
            "  num_agent_steps_trained: 5120\n",
            "  num_env_steps_sampled: 5120\n",
            "  num_env_steps_trained: 5120\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-49-05\n",
            "done: false\n",
            "episode_len_mean: 66.26388888888889\n",
            "episode_media: {}\n",
            "episode_reward_max: 295.0\n",
            "episode_reward_mean: 66.26388888888889\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 72\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.5051172971725464\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009309949353337288\n",
            "        policy_loss: -0.01547712180763483\n",
            "        total_loss: 1.0993205308914185\n",
            "        vf_explained_var: 0.6082525253295898\n",
            "        vf_loss: 1.1106081008911133\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 1152.5\n",
            "  num_agent_steps_sampled: 5120\n",
            "  num_agent_steps_trained: 5120\n",
            "  num_env_steps_sampled: 5120\n",
            "  num_env_steps_trained: 5120\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 5120\n",
            "num_agent_steps_trained: 5120\n",
            "num_env_steps_sampled: 5120\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 5120\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 54.89285714285715\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.054416498903180566\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06472155583499511\n",
            "  mean_inference_ms: 3.2868877625810695\n",
            "  mean_raw_obs_processing_ms: 0.3056735830028662\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 66.26388888888889\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 295.0\n",
            "  episode_reward_mean: 66.26388888888889\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166, 246,\n",
            "      205, 163, 191]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0, 246.0, 205.0, 163.0, 191.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.054416498903180566\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06472155583499511\n",
            "    mean_inference_ms: 3.2868877625810695\n",
            "    mean_raw_obs_processing_ms: 0.3056735830028662\n",
            "time_since_restore: 48.70761966705322\n",
            "time_this_iter_s: 9.781148433685303\n",
            "time_total_s: 48.70761966705322\n",
            "timers:\n",
            "  learn_throughput: 171.844\n",
            "  learn_time_ms: 5958.897\n",
            "  synch_weights_time_ms: 5.62\n",
            "  training_iteration_time_ms: 9735.854\n",
            "timestamp: 1674553745\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5120\n",
            "training_iteration: 5\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 6144\n",
            "counters:\n",
            "  num_agent_steps_sampled: 6144\n",
            "  num_agent_steps_trained: 6144\n",
            "  num_env_steps_sampled: 6144\n",
            "  num_env_steps_trained: 6144\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-49-14\n",
            "done: false\n",
            "episode_len_mean: 79.25974025974025\n",
            "episode_media: {}\n",
            "episode_reward_max: 355.0\n",
            "episode_reward_mean: 79.25974025974025\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 5\n",
            "episodes_total: 77\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.5021557807922363\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009048881009221077\n",
            "        policy_loss: -0.015796680003404617\n",
            "        total_loss: 0.552643895149231\n",
            "        vf_explained_var: 0.9176445007324219\n",
            "        vf_loss: 0.5643686056137085\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 1408.5\n",
            "  num_agent_steps_sampled: 6144\n",
            "  num_agent_steps_trained: 6144\n",
            "  num_env_steps_sampled: 6144\n",
            "  num_env_steps_trained: 6144\n",
            "iterations_since_restore: 6\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 6144\n",
            "num_agent_steps_trained: 6144\n",
            "num_env_steps_sampled: 6144\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 6144\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 55.0\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.05434812550899049\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06470960566629817\n",
            "  mean_inference_ms: 3.2855433519707304\n",
            "  mean_raw_obs_processing_ms: 0.30413439373901197\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 79.25974025974025\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 355.0\n",
            "  episode_reward_mean: 79.25974025974025\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 5\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166, 246,\n",
            "      205, 163, 191, 355, 182, 240, 259, 296]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0, 246.0, 205.0, 163.0, 191.0, 355.0,\n",
            "      182.0, 240.0, 259.0, 296.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.05434812550899049\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06470960566629817\n",
            "    mean_inference_ms: 3.2855433519707304\n",
            "    mean_raw_obs_processing_ms: 0.30413439373901197\n",
            "time_since_restore: 58.41469359397888\n",
            "time_this_iter_s: 9.70707392692566\n",
            "time_total_s: 58.41469359397888\n",
            "timers:\n",
            "  learn_throughput: 171.893\n",
            "  learn_time_ms: 5957.2\n",
            "  synch_weights_time_ms: 5.519\n",
            "  training_iteration_time_ms: 9729.888\n",
            "timestamp: 1674553754\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 6144\n",
            "training_iteration: 6\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 7168\n",
            "counters:\n",
            "  num_agent_steps_sampled: 7168\n",
            "  num_agent_steps_trained: 7168\n",
            "  num_env_steps_sampled: 7168\n",
            "  num_env_steps_trained: 7168\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-49-24\n",
            "done: false\n",
            "episode_len_mean: 87.67901234567901\n",
            "episode_media: {}\n",
            "episode_reward_max: 355.0\n",
            "episode_reward_mean: 87.67901234567901\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 81\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.5234394073486328\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011803937144577503\n",
            "        policy_loss: -0.01507646031677723\n",
            "        total_loss: 0.10890974849462509\n",
            "        vf_explained_var: 0.9903391599655151\n",
            "        vf_loss: 0.11867444217205048\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 1664.5\n",
            "  num_agent_steps_sampled: 7168\n",
            "  num_agent_steps_trained: 7168\n",
            "  num_env_steps_sampled: 7168\n",
            "  num_env_steps_trained: 7168\n",
            "iterations_since_restore: 7\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 7168\n",
            "num_agent_steps_trained: 7168\n",
            "num_env_steps_sampled: 7168\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 7168\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 55.26428571428572\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.05430124249703455\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06471365743458309\n",
            "  mean_inference_ms: 3.2845352836988706\n",
            "  mean_raw_obs_processing_ms: 0.30295520237008616\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 87.67901234567901\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 355.0\n",
            "  episode_reward_mean: 87.67901234567901\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166, 246,\n",
            "      205, 163, 191, 355, 182, 240, 259, 296, 252, 203, 313, 231]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0, 246.0, 205.0, 163.0, 191.0, 355.0,\n",
            "      182.0, 240.0, 259.0, 296.0, 252.0, 203.0, 313.0, 231.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.05430124249703455\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06471365743458309\n",
            "    mean_inference_ms: 3.2845352836988706\n",
            "    mean_raw_obs_processing_ms: 0.30295520237008616\n",
            "time_since_restore: 68.11669254302979\n",
            "time_this_iter_s: 9.701998949050903\n",
            "time_total_s: 68.11669254302979\n",
            "timers:\n",
            "  learn_throughput: 171.959\n",
            "  learn_time_ms: 5954.916\n",
            "  synch_weights_time_ms: 5.673\n",
            "  training_iteration_time_ms: 9724.955\n",
            "timestamp: 1674553764\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 7168\n",
            "training_iteration: 7\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 8192\n",
            "counters:\n",
            "  num_agent_steps_sampled: 8192\n",
            "  num_agent_steps_trained: 8192\n",
            "  num_env_steps_sampled: 8192\n",
            "  num_env_steps_trained: 8192\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-49-34\n",
            "done: false\n",
            "episode_len_mean: 95.38823529411765\n",
            "episode_media: {}\n",
            "episode_reward_max: 355.0\n",
            "episode_reward_mean: 95.38823529411765\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 85\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.49736833572387695\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.006834634579718113\n",
            "        policy_loss: -0.010136470198631287\n",
            "        total_loss: 0.18173256516456604\n",
            "        vf_explained_var: 0.9806084632873535\n",
            "        vf_loss: 0.18879345059394836\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 1920.5\n",
            "  num_agent_steps_sampled: 8192\n",
            "  num_agent_steps_trained: 8192\n",
            "  num_env_steps_sampled: 8192\n",
            "  num_env_steps_trained: 8192\n",
            "iterations_since_restore: 8\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 8192\n",
            "num_agent_steps_trained: 8192\n",
            "num_env_steps_sampled: 8192\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 8192\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 55.91428571428572\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.054258257289979524\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06471738806690912\n",
            "  mean_inference_ms: 3.283430662682021\n",
            "  mean_raw_obs_processing_ms: 0.3018876053577405\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 95.38823529411765\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 355.0\n",
            "  episode_reward_mean: 95.38823529411765\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166, 246,\n",
            "      205, 163, 191, 355, 182, 240, 259, 296, 252, 203, 313, 231, 217, 314, 217, 258]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0, 246.0, 205.0, 163.0, 191.0, 355.0,\n",
            "      182.0, 240.0, 259.0, 296.0, 252.0, 203.0, 313.0, 231.0, 217.0, 314.0, 217.0,\n",
            "      258.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.054258257289979524\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06471738806690912\n",
            "    mean_inference_ms: 3.283430662682021\n",
            "    mean_raw_obs_processing_ms: 0.3018876053577405\n",
            "time_since_restore: 77.71337509155273\n",
            "time_this_iter_s: 9.59668254852295\n",
            "time_total_s: 77.71337509155273\n",
            "timers:\n",
            "  learn_throughput: 172.32\n",
            "  learn_time_ms: 5942.417\n",
            "  synch_weights_time_ms: 5.617\n",
            "  training_iteration_time_ms: 9707.957\n",
            "timestamp: 1674553774\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 8192\n",
            "training_iteration: 8\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 9216\n",
            "counters:\n",
            "  num_agent_steps_sampled: 9216\n",
            "  num_agent_steps_trained: 9216\n",
            "  num_env_steps_sampled: 9216\n",
            "  num_env_steps_trained: 9216\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-49-43\n",
            "done: false\n",
            "episode_len_mean: 103.51685393258427\n",
            "episode_media: {}\n",
            "episode_reward_max: 355.0\n",
            "episode_reward_mean: 103.51685393258427\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 89\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.49694377183914185\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010962527245283127\n",
            "        policy_loss: -0.008260185830295086\n",
            "        total_loss: 0.1315813660621643\n",
            "        vf_explained_var: 0.9860535264015198\n",
            "        vf_loss: 0.13490842282772064\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 2176.5\n",
            "  num_agent_steps_sampled: 9216\n",
            "  num_agent_steps_trained: 9216\n",
            "  num_env_steps_sampled: 9216\n",
            "  num_env_steps_trained: 9216\n",
            "iterations_since_restore: 9\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 9216\n",
            "num_agent_steps_trained: 9216\n",
            "num_env_steps_sampled: 9216\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 9216\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 54.89230769230768\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.05421438345486882\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06471839946943989\n",
            "  mean_inference_ms: 3.2819684675393996\n",
            "  mean_raw_obs_processing_ms: 0.30089210078280926\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 103.51685393258427\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 355.0\n",
            "  episode_reward_mean: 103.51685393258427\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166, 246,\n",
            "      205, 163, 191, 355, 182, 240, 259, 296, 252, 203, 313, 231, 217, 314, 217, 258,\n",
            "      186, 337, 245, 337]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0, 246.0, 205.0, 163.0, 191.0, 355.0,\n",
            "      182.0, 240.0, 259.0, 296.0, 252.0, 203.0, 313.0, 231.0, 217.0, 314.0, 217.0,\n",
            "      258.0, 186.0, 337.0, 245.0, 337.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.05421438345486882\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06471839946943989\n",
            "    mean_inference_ms: 3.2819684675393996\n",
            "    mean_raw_obs_processing_ms: 0.30089210078280926\n",
            "time_since_restore: 87.33640909194946\n",
            "time_this_iter_s: 9.623034000396729\n",
            "time_total_s: 87.33640909194946\n",
            "timers:\n",
            "  learn_throughput: 172.284\n",
            "  learn_time_ms: 5943.66\n",
            "  synch_weights_time_ms: 5.588\n",
            "  training_iteration_time_ms: 9697.91\n",
            "timestamp: 1674553783\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 9216\n",
            "training_iteration: 9\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n",
            "agent_timesteps_total: 10240\n",
            "counters:\n",
            "  num_agent_steps_sampled: 10240\n",
            "  num_agent_steps_trained: 10240\n",
            "  num_env_steps_sampled: 10240\n",
            "  num_env_steps_trained: 10240\n",
            "custom_metrics: {}\n",
            "date: 2023-01-24_09-49-53\n",
            "done: false\n",
            "episode_len_mean: 108.34408602150538\n",
            "episode_media: {}\n",
            "episode_reward_max: 355.0\n",
            "episode_reward_mean: 108.34408602150538\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 93\n",
            "experiment_id: 3606565ad3324720a4e3ea98de3ed9f7\n",
            "hostname: 7ee427987719\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 127.5\n",
            "      learner_stats:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 0.0010000000474974513\n",
            "        entropy: 0.48555317521095276\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009306339547038078\n",
            "        policy_loss: -0.013915613293647766\n",
            "        total_loss: 0.4183403551578522\n",
            "        vf_explained_var: 0.8126040697097778\n",
            "        vf_loss: 0.4280681014060974\n",
            "      num_agent_steps_trained: 64.0\n",
            "      num_grad_updates_lifetime: 2432.5\n",
            "  num_agent_steps_sampled: 10240\n",
            "  num_agent_steps_trained: 10240\n",
            "  num_env_steps_sampled: 10240\n",
            "  num_env_steps_trained: 10240\n",
            "iterations_since_restore: 10\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 10240\n",
            "num_agent_steps_trained: 10240\n",
            "num_env_steps_sampled: 10240\n",
            "num_env_steps_sampled_this_iter: 1024\n",
            "num_env_steps_trained: 10240\n",
            "num_env_steps_trained_this_iter: 1024\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 1024\n",
            "perf:\n",
            "  cpu_util_percent: 55.25714285714286\n",
            "  ram_util_percent: 25.100000000000005\n",
            "pid: 402\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.054171667528658034\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.06471624507972513\n",
            "  mean_inference_ms: 3.2803425211659873\n",
            "  mean_raw_obs_processing_ms: 0.29995507212390754\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 108.34408602150538\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 355.0\n",
            "  episode_reward_mean: 108.34408602150538\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [14, 16, 19, 26, 15, 11, 26, 31, 14, 13, 15, 18, 19, 12, 13,\n",
            "      24, 33, 15, 14, 17, 15, 21, 13, 18, 12, 16, 16, 17, 32, 11, 75, 37, 23, 15,\n",
            "      19, 37, 17, 12, 21, 24, 10, 20, 22, 37, 60, 21, 23, 14, 101, 104, 85, 96, 116,\n",
            "      73, 133, 70, 73, 172, 114, 243, 295, 217, 221, 199, 67, 159, 239, 166, 246,\n",
            "      205, 163, 191, 355, 182, 240, 259, 296, 252, 203, 313, 231, 217, 314, 217, 258,\n",
            "      186, 337, 245, 337, 206, 255, 180, 222]\n",
            "    episode_reward: [14.0, 16.0, 19.0, 26.0, 15.0, 11.0, 26.0, 31.0, 14.0, 13.0, 15.0,\n",
            "      18.0, 19.0, 12.0, 13.0, 24.0, 33.0, 15.0, 14.0, 17.0, 15.0, 21.0, 13.0, 18.0,\n",
            "      12.0, 16.0, 16.0, 17.0, 32.0, 11.0, 75.0, 37.0, 23.0, 15.0, 19.0, 37.0, 17.0,\n",
            "      12.0, 21.0, 24.0, 10.0, 20.0, 22.0, 37.0, 60.0, 21.0, 23.0, 14.0, 101.0, 104.0,\n",
            "      85.0, 96.0, 116.0, 73.0, 133.0, 70.0, 73.0, 172.0, 114.0, 243.0, 295.0, 217.0,\n",
            "      221.0, 199.0, 67.0, 159.0, 239.0, 166.0, 246.0, 205.0, 163.0, 191.0, 355.0,\n",
            "      182.0, 240.0, 259.0, 296.0, 252.0, 203.0, 313.0, 231.0, 217.0, 314.0, 217.0,\n",
            "      258.0, 186.0, 337.0, 245.0, 337.0, 206.0, 255.0, 180.0, 222.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.054171667528658034\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.06471624507972513\n",
            "    mean_inference_ms: 3.2803425211659873\n",
            "    mean_raw_obs_processing_ms: 0.29995507212390754\n",
            "time_since_restore: 96.98221850395203\n",
            "time_this_iter_s: 9.645809412002563\n",
            "time_total_s: 96.98221850395203\n",
            "timers:\n",
            "  learn_throughput: 172.225\n",
            "  learn_time_ms: 5945.701\n",
            "  synch_weights_time_ms: 5.53\n",
            "  training_iteration_time_ms: 9692.071\n",
            "timestamp: 1674553793\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 10240\n",
            "training_iteration: 10\n",
            "trial_id: default\n",
            "warmup_time: 9.991425037384033\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": 'CartPole-v1',\n",
        "\t\t# \"env_config\": ENV_CONFIG,\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "    \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "    'reuse_actors':True,\n",
        "    \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "    \"callbacks\": CustomCallbacks,\n",
        "    # PPO config\n",
        "    \"use_critic\": True,\n",
        "    \"use_gae\": True,\n",
        "    \"lambda\": 1,\n",
        "    \"kl_coeff\": 0.2,\n",
        "    \"rollout_fragment_length\":1024,\n",
        "    \"train_batch_size\": 1024,\n",
        "    \"sgd_minibatch_size\": 64,\n",
        "    \"shuffle_sequences\": True,\n",
        "    \"num_sgd_iter\": 16,\n",
        "    \"lr\": 1e-3,\n",
        "    \"lr_schedule\": None,\n",
        "    \"vf_loss_coeff\": 1.0,\n",
        "    \"model\": {\n",
        "        \"vf_share_layers\": False,\n",
        "    },\n",
        "    \"entropy_coeff\": 0.0,\n",
        "    \"entropy_coeff_schedule\": None,\n",
        "    \"clip_param\": 0.4,\n",
        "    \"vf_clip_param\": 10.0,\n",
        "    \"grad_clip\": None,\n",
        "    \"kl_target\": 0.01,\n",
        "    \"batch_mode\": \"truncate_episodes\",\n",
        "    \"observation_filter\": \"NoFilter\"\n",
        "\t}\n",
        "algo = PPO(config=CONFIG)\n",
        "for k in range(10):\n",
        "\tresult=algo.train()\n",
        "\tprint(pretty_print(result))\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83xYH7vBu1KX"
      },
      "source": [
        "**Task for you**\n",
        "\n",
        "\n",
        "Now that you have seen these tools. Try to optimize the training, play with the hyper-parmeters, customize your own metrics, ... \n",
        "Make the model learn to control the CartPole."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import IPython\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "def show_animation(img, name=\"out.gif\"):\n",
        "  frames = []\n",
        "  fig = plt.figure()\n",
        "  for i in range(len(img)):\n",
        "      frames.append([plt.imshow(img[i], animated=True)])\n",
        "  anim = animation.ArtistAnimation(fig, frames)\n",
        "  #anim.save(name, writer = \"pillow\", fps=10 )\n",
        "  IPython.display.display(anim);\n",
        "  \n",
        "env = gym.make('CartPole-v1')\n",
        "episode_reward = 0\n",
        "d = False\n",
        "s = env.reset()\n",
        "k = 0\n",
        "L = []\n",
        "while not d:\n",
        "    L.append(env.render('rgb_array'))\n",
        "    logits,_= policy.model({'obs': np.expand_dims(s,axis=0)})\n",
        "    a=np.argmax(logits)\n",
        "    k =  k+1\n",
        "    s,r,d,i= env.step(a)\n",
        "    episode_reward += r\n",
        "print(k)\n",
        "env.close()\n",
        "show_animation(L)"
      ],
      "metadata": {
        "id": "FUzi8QVuQXe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVBx0CdRSpTY"
      },
      "source": [
        "# Pong with **Rainbow**\n",
        "\n",
        "Now we're going to see another algorithm on another environment\n",
        " \n",
        "Fisrt let's install the atri suite for gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kZxbBsfgVRmS"
      },
      "outputs": [],
      "source": [
        "!pip install \"gym[atari]\" \"gym[accept-rom-license]\" > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0xaKWKtqR9V"
      },
      "source": [
        "Normally you need to : **RESTART RUNTIME**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uim6g5_Z3_U"
      },
      "source": [
        "Let's check our new environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b-SoHF7OUgmW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "d3cd1299-371b-43c2-bbe7-f1f625f013ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gym\n",
        "import ray\n",
        "from pyvirtualdisplay import Display\n",
        "import pygame\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from ray_course.gym_recorder import *\n",
        "gymlogger.set_level(40) #error only\n",
        "display = Display(visible=0, size=(1400, 900)) #display\n",
        "display.start()\n",
        "env=wrap_env(gym.make('PongDeterministic-v0')\n",
        ") #env\n",
        "# rollout\n",
        "s = env.reset()\n",
        "print(s.shape)\n",
        "d=False\n",
        "while not d:\n",
        "    env.render('rgb_array')\n",
        "    a = env.action_space.sample() \n",
        "    s, r, d, i = env.step(a) \n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCCKLg-xZDHC"
      },
      "source": [
        "Useful links for Rainbow algorithm: \n",
        "\n",
        "* [Rainbow article](https://arxiv.org/abs/1710.02298)\n",
        "* [Rainbow on RLlib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#dqn)\n",
        "\n",
        "\n",
        "Now let's see if we can learn a good policy with rainbow. Here is the new CONFIG and the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "br0-54xVaRR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64489064-9b13-4a09-b8ff-0c0131eb76c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ray.rllib.utils.compression:lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "2023-01-24 10:03:54,894\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "2023-01-24 10:03:56,286\tINFO algorithm_config.py:2492 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
            "2023-01-24 10:03:56,323\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=16909)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=16909)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=16909)\u001b[0m 2023-01-24 10:04:01,051\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=16909)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=16909)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment PongDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m   logger.warn(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m [Powered by Stella]\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m 2023-01-24 10:04:02,335\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m   logger.deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m 2023-01-24 10:04:02,351\tWARNING deprecation.py:47 -- DeprecationWarning: `FrameStack` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=16909)\u001b[0m 2023-01-24 10:04:03.004862: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvalentin-thoraval\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230124_100412-zf66l8uh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/valentin-thoraval/RLlib/runs/zf66l8uh\" target=\"_blank\">fortuitous-dragon-2</a></strong> to <a href=\"https://wandb.ai/valentin-thoraval/RLlib\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/valentin-thoraval/RLlib\" target=\"_blank\">https://wandb.ai/valentin-thoraval/RLlib</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/valentin-thoraval/RLlib/runs/zf66l8uh\" target=\"_blank\">https://wandb.ai/valentin-thoraval/RLlib/runs/zf66l8uh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 10:04:14,593\tINFO trainable.py:172 -- Trainable.setup took 18.272 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-01-24 10:04:14,598\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
            "2023-01-24 10:04:14,744\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  0\n",
            "epoch :  1\n",
            "epoch :  2\n",
            "epoch :  3\n",
            "epoch :  4\n",
            "epoch :  5\n",
            "epoch :  6\n",
            "epoch :  7\n",
            "epoch :  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 10:06:31,459\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  9\n",
            "epoch :  10\n",
            "epoch :  11\n",
            "epoch :  12\n",
            "epoch :  13\n",
            "epoch :  14\n",
            "epoch :  15\n",
            "epoch :  16\n",
            "epoch :  17\n",
            "epoch :  18\n",
            "epoch :  19\n",
            "epoch :  20\n",
            "epoch :  21\n",
            "epoch :  22\n",
            "epoch :  23\n",
            "epoch :  24\n",
            "epoch :  25\n",
            "epoch :  26\n",
            "epoch :  27\n",
            "epoch :  28\n",
            "epoch :  29\n",
            "epoch :  30\n",
            "epoch :  31\n",
            "epoch :  32\n",
            "epoch :  33\n",
            "epoch :  34\n",
            "epoch :  35\n",
            "epoch :  36\n",
            "epoch :  37\n",
            "epoch :  38\n",
            "epoch :  39\n",
            "epoch :  40\n",
            "epoch :  41\n",
            "epoch :  42\n",
            "epoch :  43\n",
            "epoch :  44\n",
            "epoch :  45\n",
            "epoch :  46\n",
            "epoch :  47\n",
            "epoch :  48\n",
            "epoch :  49\n",
            "epoch :  50\n",
            "epoch :  51\n",
            "epoch :  52\n",
            "epoch :  53\n",
            "epoch :  54\n",
            "epoch :  55\n",
            "epoch :  56\n",
            "epoch :  57\n",
            "epoch :  58\n",
            "epoch :  59\n",
            "epoch :  60\n",
            "epoch :  61\n",
            "epoch :  62\n",
            "epoch :  63\n",
            "epoch :  64\n",
            "epoch :  65\n",
            "epoch :  66\n",
            "epoch :  67\n",
            "epoch :  68\n",
            "epoch :  69\n",
            "epoch :  70\n",
            "epoch :  71\n",
            "epoch :  72\n",
            "epoch :  73\n",
            "epoch :  74\n",
            "epoch :  75\n",
            "epoch :  76\n",
            "epoch :  77\n",
            "epoch :  78\n",
            "epoch :  79\n",
            "epoch :  80\n",
            "epoch :  81\n",
            "epoch :  82\n",
            "epoch :  83\n",
            "epoch :  84\n",
            "epoch :  85\n",
            "epoch :  86\n",
            "epoch :  87\n",
            "epoch :  88\n",
            "epoch :  89\n",
            "epoch :  90\n",
            "epoch :  91\n",
            "epoch :  92\n",
            "epoch :  93\n",
            "epoch :  94\n",
            "epoch :  95\n",
            "epoch :  96\n",
            "epoch :  97\n",
            "epoch :  98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 10:54:40,666\tWARNING policy.py:121 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.DQNTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  99\n",
            "Checkpoint saved in directory /root/ray_results/DQN_PongDeterministic-v0_2023-01-24_10-03-56wbicmczu/checkpoint_000100\n"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": 'PongDeterministic-v0',\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "        \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "        \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "        \"callbacks\": CustomCallbacks,\n",
        "        # rainbow config\n",
        "        'env_config':{},  # deterministic\n",
        "        'num_gpus': 1,\n",
        "        'gamma': 0.99,\n",
        "        'lr': .0001,\n",
        "        'replay_buffer_config':\n",
        "            {'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "            'capacity': 50000},\n",
        "        'num_steps_sampled_before_learning_starts': 10000,\n",
        "        'rollout_fragment_length': 4,\n",
        "        'train_batch_size' : 32,\n",
        "        'exploration_config' :\n",
        "            {'epsilon_timesteps': 200000,\n",
        "            'final_epsilon': .01},\n",
        "        'model':\n",
        "            {'grayscale': True,\n",
        "            'zero_mean': False,\n",
        "            'dim': 42},\n",
        "        # we should set compress_observations to True because few machines\n",
        "        # would be able to contain the replay buffers in memory otherwise\n",
        "        'compress_observations' : True,\n",
        "            }\n",
        "algo = DQN(config=CONFIG)\n",
        "now = datetime.now()\n",
        "name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
        "wandb.run.name='rainbow'+name\n",
        "for k in range(100):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',k)\n",
        "    # print(pretty_print(result))\n",
        " \n",
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pMMJqNzbYOh"
      },
      "source": [
        "## Custom Environment\n",
        "\n",
        "Cool! It works but as you can see the state is defined as the image at the given time step (just like when you play ! ). However, when learning on Atari it is more efficient to define the state of the agent as a stack of frame so that it is aware of the dynamic (like the speed vector of the ball). Otherwise it won't learn much.\n",
        "\n",
        "When you call RLlib with Atari environment, RLlib recognizes the Atari environment and prepocess it with [DeepMind Preprocessor](https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/atari_wrappers.py) so that the state become the stack of frames.\n",
        "\n",
        "Now we are going to see how to wrap an Atari environment so that the state is defined as stack of num_frames=3. Also, instead of having the state being an rgb images we will set it as a gray scale image. \n",
        "Summary : we had observation_space.shape=(1,height, width, C) C=3 for RGB channel, and now we have exactly the same space but the three channels are the 3 frames.\n",
        "\n",
        "Let see how to do that :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8-B1bQnR8roC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2293dfe5-1cb2-4d78-8869-9041798842a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-24 10:55:33,019\tINFO worker.py:1538 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box(0, 255, (84, 84, 3), uint8)\n",
            "(84, 84, 3)\n",
            "(84, 84, 3)\n"
          ]
        }
      ],
      "source": [
        "from numpy.core.memmap import uint8\n",
        "import numpy as np \n",
        "import gym \n",
        "import cv2\n",
        "import numpy as np\n",
        "from gym.spaces import Box\n",
        "import matplotlib.pyplot as plt \n",
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self,dict_env={}) -> None:\n",
        "        super(CustomEnv, self).__init__()\n",
        "        self.dict_env=dict_env\n",
        "        self.old_env= gym.make('PongDeterministic-v0')\n",
        "        self.num_frames=3\n",
        "        self.shape_new_image=(84, 84, 1)\n",
        "        self.observation_space=Box(0, 255, (self.shape_new_image[0], self.shape_new_image[1], self.shape_new_image[2]*3), dtype=uint8)\n",
        "        self.action_space=self.old_env.action_space\n",
        "        self.seq_s=[np.zeros(shape=self.shape_new_image) for _ in range(self.num_frames)]\n",
        "        \n",
        "    def reset(self):\n",
        "        # self.seq_s=[np.zeros(shape=self.shape_new_image) for _ in range(self.num_frames)] #reset sequence of states\n",
        "        old_env_s= self.old_env.reset() #state from the old environment\n",
        "        old_env_s=self.image_filter(np.array(old_env_s))\n",
        "        self.seq_s.pop(0) #delete older\n",
        "        self.seq_s.append(old_env_s)\n",
        "        s = np.concatenate(self.seq_s,axis=-1) \n",
        "        return s\n",
        "\n",
        "    def step(self, a):\n",
        "        old_env_s, r, d, i = self.old_env.step(a) \n",
        "        old_env_s=self.image_filter(np.array(old_env_s))\n",
        "        self.seq_s.pop(0)\n",
        "        self.seq_s.append(old_env_s)\n",
        "        s = np.concatenate(self.seq_s,axis=-1)\n",
        "        return s, r, d, i\n",
        "\n",
        "    def render(self,args):\n",
        "        self.old_env.render(args)\n",
        "    \n",
        "    def image_filter(self,img):\n",
        "        img_gs= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        resized = cv2.resize(img_gs, self.shape_new_image[:-1], interpolation = cv2.INTER_AREA)/255.0 #normalize the data\n",
        "        return np.expand_dims(resized,axis=-1)\n",
        "\n",
        "    def __reduce__(self):\n",
        "        deserializer = CustomEnv\n",
        "        serialized_data = (self.dict_env, )\n",
        "        return deserializer, serialized_data\n",
        "\n",
        "\n",
        "\n",
        "env=CustomEnv()\n",
        "id=ray.put(env) #check if it can be serialized\n",
        "print(env.observation_space)\n",
        "s=env.reset()\n",
        "print(s.shape)\n",
        "# plt.imshow(s)\n",
        "s, r, d, i = env.step(env.action_space.sample())\n",
        "print(s.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KB1-JervDeM"
      },
      "source": [
        "# Change your model\n",
        "\n",
        "Now that we have a new state let's also implement our own model.\n",
        "As we saw, ray has several API levels. The very high level with Tune and lower levels.\n",
        "When implementing a RL algo it is essential to change the structure of the model.\n",
        "To do this we will modify the ModelTF2 class (feel free to use the [pytorch class](https://docs.ray.io/en/latest/rllib/rllib-models.html#custom-pytorch-models))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTioOdekwLxm"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
        "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.tf.misc import normc_initializer\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.visionnet import VisionNetwork as MyVisionNetwork\n",
        "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
        "from ray.rllib.utils.framework import try_import_tf\n",
        "from ray.rllib.utils.metrics.learner_info import LEARNER_INFO, LEARNER_STATS_KEY\n",
        "from ray.tune.registry import get_trainable_cls\n",
        "tf1, tf, tfv = try_import_tf()\n",
        "class AtariModel(TFModelV2):\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
        "                 name=\"atari_model\"):\n",
        "        super(AtariModel, self).__init__(obs_space, action_space, num_outputs, model_config,\n",
        "                         name)\n",
        "        inputs = tf.keras.layers.Input(shape=obs_space.shape, name='observations')\n",
        "        # Convolutions on the frames on the screen\n",
        "        layer1 = tf.keras.layers.Conv2D(\n",
        "                32,\n",
        "                [8, 8],\n",
        "                strides=(4, 4),\n",
        "                activation=\"relu\",\n",
        "                data_format='channels_last')(inputs)\n",
        "        layer2 = tf.keras.layers.Conv2D(\n",
        "                32,\n",
        "                [4, 4],\n",
        "                strides=(2, 2),\n",
        "                activation=\"relu\",\n",
        "                data_format='channels_last')(layer1)\n",
        "        layer3 = tf.keras.layers.Conv2D(\n",
        "                32,\n",
        "                [3, 3],\n",
        "                strides=(1, 1),\n",
        "                activation=\"relu\",\n",
        "                data_format='channels_last')(layer2)\n",
        "        layer4 = tf.keras.layers.Flatten()(layer3)\n",
        "        layer5 = tf.keras.layers.Dense(\n",
        "                512,\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=normc_initializer(1.0))(layer4)\n",
        "        action = tf.keras.layers.Dense(\n",
        "                num_outputs,\n",
        "                activation=\"linear\",\n",
        "                name=\"actions\",\n",
        "                kernel_initializer=normc_initializer(0.01))(layer5)\n",
        "        self.base_model = tf.keras.Model(inputs, action)\n",
        "        # self.register_variables(self.base_model.variables)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.base_model(input_dict[\"obs\"])\n",
        "        return model_out, state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgijl-M_9lpb"
      },
      "source": [
        "now you can register the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UggXMnIB9ksS"
      },
      "outputs": [],
      "source": [
        "ray.init()\n",
        "ModelCatalog.register_custom_model(\"AtariModel\", AtariModel)\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-DPZDRn97eY"
      },
      "source": [
        "Now let's use your own model with your CustomEnv in the CONFIG dict: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6BPlGbE-AMC"
      },
      "outputs": [],
      "source": [
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "import ray \n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": CustomEnv,\n",
        "\t\t# \"env_config\": ENV_CONFIG,\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "        \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "        \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "        \"callbacks\": CustomCallbacks,\n",
        "        # rainbow config\n",
        "        'env_config':{},  # deterministic\n",
        "        'num_gpus': 1,\n",
        "        'gamma': 0.99,\n",
        "        'lr': .0001,\n",
        "        'replay_buffer_config':\n",
        "            {'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "            'capacity': 50000},\n",
        "        'num_steps_sampled_before_learning_starts': 1000,\n",
        "        'rollout_fragment_length': 4,\n",
        "        'train_batch_size' : 32,\n",
        "        'exploration_config' :\n",
        "            {'epsilon_timesteps': 200000,\n",
        "            'final_epsilon': .01},\n",
        "        'model':\n",
        "            {'custom_model': AtariModel,\n",
        "            'grayscale': True,\n",
        "            'zero_mean': False,\n",
        "            'dim': 42},\n",
        "        # we should set compress_observations to True because few machines\n",
        "        # would be able to contain the replay buffers in memory otherwise\n",
        "        'compress_observations' : True,\n",
        "            }\n",
        "algo = DQN(config=CONFIG)\n",
        "now = datetime.now()\n",
        "name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
        "wandb.run.name='rainbow_custom_model'+name\n",
        "for k in range(10):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',k)\n",
        "    # print(pretty_print(result))\n",
        " \n",
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpEspwE4EL-4"
      },
      "source": [
        "# Change the Loss (Implementing CURL)\n",
        "\n",
        "Ok so we've seen how to customize the hyper-parameters, how to pass a specific environment to the algorithm, and how to customize your own estimator. \n",
        "One important thing left is how to change the loss. Once, you've seen this last feature you will be able to implement almost all the DRL algorithms with RLlib.\n",
        "\n",
        "**CURL (Contrastive Unsupervised Representations for Reinforcement Learning)**\n",
        "\n",
        "Rainbow is great, but we can be more efficient. More specifically, we can be sample efficient. In order to make our algorithm more sample efficient we're going to use contrastive learning.\n",
        "\n",
        "**Contrastive Learning** :\n",
        "In contrastive learning, the main idea is to train a neural network to learn representations of data that are \"close\" for similar data points, and \"far\" for different data points. This is achieved by defining a distance metric between the representations of the input data in a feature space, and then training the network to minimize the distance between the representations of similar data points and maximize the distance between the representations of dissimilar data points. The distance between the representations is commonly measured by a contrastive loss function. This approach aims to learn useful and informative features from the input data.\n",
        "\n",
        "Useful links :\n",
        "\n",
        "* [Blog on Contrastive Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)\n",
        "* [Curl paper](https://arxiv.org/abs/2004.04136)\n",
        "* [Video summarizing Curl](https://www.youtube.com/watch?v=-Drowt9r4zY)\n",
        "\n",
        "For this we are going to use the Policy API which allows you to change the loss functions, the actions computations, ... see [Policy API](https://docs.ray.io/en/latest/rllib/package_ref/policy/policy.html)\n",
        "\n",
        "We will implement the algorithm in 4 steps : \n",
        "\n",
        "* Build the model for Curl (encoder q,k,...)\n",
        "* Implement the custom loss\n",
        "* Build the policy with the new loss \n",
        "* Modify the training step method of the algorithm to make the EMA (exponential moving average) update\n",
        "\n",
        "Again i will be using tensorflow ([here is the pytorch alternative](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#building-policies-in-tensorflow-eager)): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2vAm5dioux"
      },
      "source": [
        "1. First we need to change the model so that we have the key encoder and the query encoder. Also we can defined the function that will allow us to make the EMA update of the k encoder in the direction of the q encoder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Q4rBjGHl65"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
        "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.tf.misc import normc_initializer\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.visionnet import VisionNetwork as MyVisionNetwork\n",
        "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
        "from ray.rllib.utils.framework import try_import_tf\n",
        "from ray.rllib.utils.metrics.learner_info import LEARNER_INFO, LEARNER_STATS_KEY\n",
        "from ray.tune.registry import get_trainable_cls\n",
        "tf1, tf, tfv = try_import_tf()\n",
        "\n",
        "class AtariModel(TFModelV2):\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
        "                 name=\"atari_model\"):\n",
        "        super(AtariModel, self).__init__(obs_space, action_space, num_outputs, model_config,\n",
        "                         name)\n",
        "        inputs = tf.keras.layers.Input(shape=obs_space.shape, name='observations')\n",
        "        self.beta=0.9\n",
        "        # Convolutions on the frames on the screen\n",
        "        # data aug layer k\n",
        "        # layer_aug_k_flip=tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(inputs)\n",
        "        # layer_aug_k=tf.keras.layers.RandomRotation(0.2)(layer_aug_k_flip)\n",
        "        #encoder keys\n",
        "        layer_k_1 = tf.keras.layers.Conv2D(32,[8, 8],strides=(4, 4),activation=\"relu\",data_format='channels_last',name='layer_k_1',trainable=False)(inputs)\n",
        "        layer_k_2 = tf.keras.layers.Conv2D(64,[4, 4],strides=(2, 2),activation=\"relu\",data_format='channels_last',name='layer_k_2',trainable=False)(layer_k_1)\n",
        "        layer_k_3 = tf.keras.layers.Conv2D(64,[3, 3],strides=(1, 1),activation=\"relu\",data_format='channels_last',name='layer_k_3',trainable=False)(layer_k_2)\n",
        "        flatten_k = tf.keras.layers.Flatten(name='layer_k_4',trainable=False)(layer_k_3)\n",
        "        encoder_k= tf.keras.layers.Dense(1024,activation=\"relu\",kernel_initializer=normc_initializer(1.0), name='layer_k_5')(flatten_k)\n",
        "        # data aug layer q\n",
        "        # layer_aug_q_flip=tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(inputs)\n",
        "        # layer_aug_q=tf.keras.layers.RandomRotation(0.2)(layer_aug_q_flip)\n",
        "        #encoder queries\n",
        "        layer_q_1 = tf.keras.layers.Conv2D(32,[8, 8],strides=(4, 4),activation=\"relu\",data_format='channels_last',name='layer_q_1')(inputs)\n",
        "        layer_q_2 = tf.keras.layers.Conv2D(64,[4, 4],strides=(2, 2),activation=\"relu\",data_format='channels_last',name='layer_q_2')(layer_q_1)\n",
        "        layer_q_3 = tf.keras.layers.Conv2D(64,[3, 3],strides=(1, 1),activation=\"relu\",data_format='channels_last',name='layer_q_3')(layer_q_2)\n",
        "        flatten_q = tf.keras.layers.Flatten(name='layer_q_4')(layer_q_3)\n",
        "        encoder_q = tf.keras.layers.Dense(1024,activation=\"relu\",kernel_initializer=normc_initializer(1.0), name='layer_q_5')(flatten_q)\n",
        "        # Project Matrix W\n",
        "        # W=tf.keras.layers.Dense(64, activation=None, use_bias=False)(encoder_k)\n",
        "        # Head layer\n",
        "        layer5 = tf.keras.layers.Dense(512,activation=\"relu\",kernel_initializer=normc_initializer(1.0))(encoder_q)\n",
        "        action = tf.keras.layers.Dense(num_outputs,activation=\"linear\",name=\"actions\",kernel_initializer=normc_initializer(0.01))(layer5)\n",
        "        self.base_model = tf.keras.Model(inputs, [action, encoder_q,encoder_k])\n",
        "\n",
        "        # self.register_variables(self.base_model.variables)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out, self._encoder_q_out, self._encoder_k_out= self.base_model(input_dict[\"obs\"])\n",
        "        return model_out, state\n",
        "    \n",
        "    # def projection_function(self):\n",
        "    #     return self._W\n",
        "    \n",
        "    def encoder_q_function(self):\n",
        "        return self._encoder_q_out\n",
        "\n",
        "    def encoder_k_function(self):\n",
        "        return self._encoder_k_out\n",
        "\n",
        "\n",
        "    def ema_update(self):\n",
        "        for k in range(1,5):\n",
        "            layer_k=self.base_model.get_layer('layer_k_'+str(k))\n",
        "            layer_q=self.base_model.get_layer('layer_q_'+str(k))\n",
        "            weights_k_l=layer_k.get_weights()\n",
        "            weights_q_l=layer_q.get_weights()\n",
        "            new_weights_k=[]\n",
        "            for i in range(len(weights_k_l)):\n",
        "                new_weights_k.append(weights_k_l[i]*float(self.beta)+(1-float(self.beta))*weights_q_l[i])\n",
        "            self.base_model.get_layer('layer_k_'+str(k)).set_weights(new_weights_k)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XCbESaUs6rC"
      },
      "source": [
        "2. Now let's modify the initial loss to add the contrastive loss in the loss_fn function. The image augmentation is defined just like in the article (random_crop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5ID6EUsL66E"
      },
      "outputs": [],
      "source": [
        "from torch import logit\n",
        "from ray.rllib.algorithms.dqn.dqn_tf_policy import build_q_losses\n",
        "from ray.rllib.policy.sample_batch import SampleBatch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CosineSimilarity\n",
        "\n",
        "\n",
        "def custom_loss(policy, model, _, train_batch):\n",
        "    # RAINBOW LOSS\n",
        "    dqn_loss= build_q_losses(policy, model, _, train_batch)\n",
        "\n",
        "    # OUR CONTRASTIVE LOSS\n",
        "    cosine_loss = tf.keras.losses.CosineSimilarity(axis=-1)\n",
        "    size_crop=20\n",
        "    input_batch=train_batch[SampleBatch.CUR_OBS]\n",
        "    # augmented view for query \n",
        "    i_x=np.random.randint(0,size_crop)\n",
        "    i_y=np.random.randint(0,size_crop)\n",
        "    cropped_s= tf.keras.layers.Cropping2D(cropping=((i_x, size_crop-i_x), (i_y, size_crop-i_y)))(input_batch)\n",
        "    augmented_q = tf.image.resize(cropped_s, (input_batch.shape[1], input_batch.shape[2]), method = tf.image.ResizeMethod.GAUSSIAN) #gausian interpolation\n",
        "    model_output, _ = model({'obs' : augmented_q})\n",
        "    # latent q\n",
        "    z_q=model.encoder_q_function()\n",
        "    # augmented view for key \n",
        "    i_x=np.random.randint(0,size_crop)\n",
        "    i_y=np.random.randint(0,size_crop)\n",
        "    cropped_s= tf.keras.layers.Cropping2D(cropping=((i_x, size_crop-i_x), (i_y, size_crop-i_y)))(input_batch)\n",
        "    augmented_k = tf.image.resize(cropped_s, (input_batch.shape[1], input_batch.shape[2]), method = tf.image.ResizeMethod.GAUSSIAN)\n",
        "    model_output, _ = model({'obs' : augmented_k})\n",
        "    # latent k\n",
        "    z_k=model.encoder_k_function()\n",
        "    # contrastive loss\n",
        "    pos_pairs_loss=cosine_loss(z_k,z_q)\n",
        "    neg_pairs_loss=0\n",
        "    for i in range(input_batch.shape[0]):\n",
        "        neg=tf.concat((z_k[:i],z_k[i+1:]),axis=0) #neg pairs\n",
        "        anchor=tf.repeat(tf.expand_dims(z_q[i],axis=0), repeats=neg.shape[0], axis=0)\n",
        "        neg_pairs_loss+=-cosine_loss(neg,anchor)\n",
        "    contrastive_loss=pos_pairs_loss+neg_pairs_loss\n",
        "    print('contrastive_loss : ',contrastive_loss)\n",
        "    print('dqn_loss : ',dqn_loss)\n",
        "    return dqn_loss + contrastive_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2VqTHcrzhBG"
      },
      "source": [
        "3. Build the policy. As you can see we can modify the initial policy with the **with_updates** function (really cool feature) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCQs2amfIwM_"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.policy.tf_policy_template import build_tf_policy\n",
        "from ray.rllib.algorithms.dqn.dqn_tf_policy import DQNTFPolicy\n",
        "\n",
        "CustomPolicy = DQNTFPolicy.with_updates(\n",
        "    name=\"CustomDQNPolicy\",\n",
        "    loss_fn=custom_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKXCBB1FWuM"
      },
      "source": [
        "4. As it can be seen in the article. After each update of to minimize the loss, there is an ema update of k toward q.\n",
        "We already implemented the ema_function. Now we need to call it in the algorithm flow. \n",
        "We are going to inherite from the DQN algorithm so that you see the structure of the **training_step** function that we are going to modify. \n",
        "Alternatively, we could have modify the **learn_on_batch** function of the policy.\n",
        "I just copy/pasted the **training_step** method of the dqn algorithm available [here](https://github.com/ray-project/ray/blob/ec3243d78726a2840f1323f997a210d1f33e5656/rllib/algorithms/dqn/dqn.py) and overided the function.\n",
        "The modification can be seen at the *******custom update***** comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1GCK-mwFVAk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "from typing import List, Optional, Type, Callable\n",
        "import numpy as np\n",
        "\n",
        "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
        "from ray.rllib.algorithms.dqn.dqn_tf_policy import DQNTFPolicy\n",
        "from ray.rllib.algorithms.dqn.dqn_torch_policy import DQNTorchPolicy\n",
        "from ray.rllib.algorithms.simple_q.simple_q import (\n",
        "    SimpleQ,\n",
        "    SimpleQConfig,\n",
        ")\n",
        "from ray.rllib.execution.rollout_ops import (\n",
        "    synchronous_parallel_sample,\n",
        ")\n",
        "from ray.rllib.policy.sample_batch import MultiAgentBatch\n",
        "from ray.rllib.execution.train_ops import (\n",
        "    train_one_step,\n",
        "    multi_gpu_train_one_step,\n",
        ")\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
        "from ray.rllib.utils.typing import ResultDict\n",
        "from ray.rllib.utils.metrics import (\n",
        "    NUM_ENV_STEPS_SAMPLED,\n",
        "    NUM_AGENT_STEPS_SAMPLED,\n",
        ")\n",
        "from ray.rllib.utils.deprecation import (\n",
        "    Deprecated,\n",
        ")\n",
        "from ray.rllib.utils.metrics import SYNCH_WORKER_WEIGHTS_TIMER\n",
        "from ray.rllib.execution.common import (\n",
        "    LAST_TARGET_UPDATE_TS,\n",
        "    NUM_TARGET_UPDATES,\n",
        ")\n",
        "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
        "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "\n",
        "def calculate_rr_weights(config: AlgorithmConfig) -> List[float]:\n",
        "    \"\"\"Calculate the round robin weights for the rollout and train steps\"\"\"\n",
        "    if not config[\"training_intensity\"]:\n",
        "        return [1, 1]\n",
        "\n",
        "    # Calculate the \"native ratio\" as:\n",
        "    # [train-batch-size] / [size of env-rolled-out sampled data]\n",
        "    # This is to set freshly rollout-collected data in relation to\n",
        "    # the data we pull from the replay buffer (which also contains old\n",
        "    # samples).\n",
        "    native_ratio = config[\"train_batch_size\"] / (\n",
        "        config.get_rollout_fragment_length()\n",
        "        * config[\"num_envs_per_worker\"]\n",
        "        # Add one to workers because the local\n",
        "        # worker usually collects experiences as well, and we avoid division by zero.\n",
        "        * max(config[\"num_workers\"] + 1, 1)\n",
        "    )\n",
        "\n",
        "    # Training intensity is specified in terms of\n",
        "    # (steps_replayed / steps_sampled), so adjust for the native ratio.\n",
        "    sample_and_train_weight = config[\"training_intensity\"] / native_ratio\n",
        "    if sample_and_train_weight < 1:\n",
        "        return [int(np.round(1 / sample_and_train_weight)), 1]\n",
        "    else:\n",
        "        return [1, int(np.round(sample_and_train_weight))]\n",
        "\n",
        "class Curl(DQN):\n",
        "    def __init__(self, config):\n",
        "        super(DQN, self).__init__(config)\n",
        "    @override(DQN)\n",
        "    def training_step(self) -> ResultDict:\n",
        "        \"\"\"DQN training iteration function.\n",
        "        Each training iteration, we:\n",
        "        - Sample (MultiAgentBatch) from workers.\n",
        "        - Store new samples in replay buffer.\n",
        "        - Sample training batch (MultiAgentBatch) from replay buffer.\n",
        "        - Learn on training batch.\n",
        "        - Update remote workers' new policy weights.\n",
        "        - Update target network every `target_network_update_freq` sample steps.\n",
        "        - Return all collected metrics for the iteration.\n",
        "        Returns:\n",
        "            The results dict from executing the training iteration.\n",
        "        \"\"\"\n",
        "        train_results = {}\n",
        "\n",
        "        # We alternate between storing new samples and sampling and training\n",
        "        store_weight, sample_and_train_weight = calculate_rr_weights(self.config)\n",
        "\n",
        "        for _ in range(store_weight):\n",
        "            # Sample (MultiAgentBatch) from workers.\n",
        "            new_sample_batch = synchronous_parallel_sample(\n",
        "                worker_set=self.workers, concat=True\n",
        "            )\n",
        "\n",
        "            # Update counters\n",
        "            self._counters[NUM_AGENT_STEPS_SAMPLED] += new_sample_batch.agent_steps()\n",
        "            self._counters[NUM_ENV_STEPS_SAMPLED] += new_sample_batch.env_steps()\n",
        "\n",
        "            # Store new samples in replay buffer.\n",
        "            self.local_replay_buffer.add(new_sample_batch)\n",
        "\n",
        "        global_vars = {\n",
        "            \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
        "        }\n",
        "\n",
        "        # Update target network every `target_network_update_freq` sample steps.\n",
        "        cur_ts = self._counters[\n",
        "            NUM_AGENT_STEPS_SAMPLED\n",
        "            if self.config.count_steps_by == \"agent_steps\"\n",
        "            else NUM_ENV_STEPS_SAMPLED\n",
        "        ]\n",
        "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
        "            for _ in range(sample_and_train_weight):\n",
        "                # Sample training batch (MultiAgentBatch) from replay buffer.\n",
        "                train_batch = sample_min_n_steps_from_buffer(\n",
        "                    self.local_replay_buffer,\n",
        "                    self.config.train_batch_size,\n",
        "                    count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
        "                )\n",
        "\n",
        "                # Postprocess batch before we learn on it\n",
        "                post_fn = self.config.get(\"before_learn_on_batch\") or (lambda b, *a: b)\n",
        "                train_batch = post_fn(train_batch, self.workers, self.config)\n",
        "\n",
        "                # for policy_id, sample_batch in train_batch.policy_batches.items():\n",
        "                #     print(len(sample_batch[\"obs\"]))\n",
        "                #     print(sample_batch.count)\n",
        "\n",
        "                # Learn on training batch.\n",
        "                # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
        "                # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
        "                if self.config.get(\"simple_optimizer\") is True:\n",
        "                    train_results = train_one_step(self, train_batch)\n",
        "                else:\n",
        "                    train_results = multi_gpu_train_one_step(self, train_batch)\n",
        "\n",
        "                # Update replay buffer priorities.\n",
        "                update_priorities_in_replay_buffer(\n",
        "                    self.local_replay_buffer,\n",
        "                    self.config,\n",
        "                    train_batch,\n",
        "                    train_results,\n",
        "                )\n",
        "                #*****************************************custom update*****************************************\n",
        "                for pid in self.workers.local_worker().policy_map.keys():\n",
        "                    self.workers.local_worker().policy_map[pid].model.ema_update()\n",
        "                #*****************************************end of the custom update*****************************************\n",
        "                last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
        "                if cur_ts - last_update >= self.config.target_network_update_freq:\n",
        "                    to_update = self.workers.local_worker().get_policies_to_train()\n",
        "                    self.workers.local_worker().foreach_policy_to_train(\n",
        "                        lambda p, pid: pid in to_update and p.update_target()\n",
        "                    )\n",
        "                    self._counters[NUM_TARGET_UPDATES] += 1\n",
        "                    self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
        "                # print(\"Worker dict : \",self.workers.local_worker().policy_dict())\n",
        "                # Update weights and global_vars - after learning on the local worker -\n",
        "                # on all remote workers.\n",
        "                with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
        "                    self.workers.sync_weights(global_vars=global_vars)\n",
        "\n",
        "        # Return all collected metrics for the iteration.\n",
        "        return train_results\n",
        "\n",
        "    def get_default_policy_class(self, config):\n",
        "        return CustomPolicy\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UXiTqsZHtsw"
      },
      "source": [
        "Now let's train Curl !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrKPTS9AHvrK"
      },
      "outputs": [],
      "source": [
        "import ray \n",
        "from ray_course.custom_callbacks import CustomCallbacks\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init() #re-init\n",
        "CONFIG = {\n",
        "    #COMON config\n",
        "\t\t\"env\": CustomEnv,\n",
        "\t\t# \"env_config\": ENV_CONFIG,\n",
        "\t\t\"num_gpus\": 0,\n",
        "\t\t\"num_workers\": 1, # int(ressources['CPU'])\n",
        "        \"gamma\": 0.95,\n",
        "\t\t\"explore\": True,\n",
        "\t\t\"exploration_config\": {\n",
        "\t\t\t\"type\": \"StochasticSampling\",\n",
        "\t\t},\n",
        "        \"framework\": \"tf2\", #I prefer tensorflow but feel free to use pytorch\n",
        "        # \"callbacks\": CustomCallbacks,\n",
        "        # rainbow config\n",
        "        'env_config':{},  # deterministic\n",
        "        'gamma': 0.99,\n",
        "        'lr': .0001,\n",
        "        'replay_buffer_config':\n",
        "            {'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "            'capacity': 50000},\n",
        "        'num_steps_sampled_before_learning_starts': 500, #10000\n",
        "        'rollout_fragment_length': 4,\n",
        "        'train_batch_size' : 32,\n",
        "        'exploration_config' :\n",
        "            {'epsilon_timesteps': 200000,\n",
        "            'final_epsilon': .01},\n",
        "        'model':\n",
        "            {'custom_model': AtariModel,\n",
        "            'grayscale': True,\n",
        "            'zero_mean': False,\n",
        "            'dim': 42},\n",
        "        # we should set compress_observations to True because few machines\n",
        "        # would be able to contain the replay buffers in memory otherwise\n",
        "        'compress_observations' : True,\n",
        "            }\n",
        "algo = Curl(config=CONFIG)\n",
        "# now = datetime.now()\n",
        "# name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
        "# wandb.run.name='rainbow_custom_model'+name\n",
        "for k in range(2):\n",
        "\tresult=algo.train()\n",
        "\tprint('epoch : ',k)\n",
        "    # print(pretty_print(result))\n",
        " \n",
        "checkpoint_dir = algo.save() #save the model \n",
        "print(f\"Checkpoint saved in directory {checkpoint_dir}\") \n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUEbk8MIepXT"
      },
      "source": [
        "**Questions** :\n",
        "\n",
        "There is an hyper-parameter really important in order for curl to work, can you guess which one it is ? \n",
        "\n",
        "We made an error in our implementation, can you guess what it is ? (indication : it's in the loss definition)\n",
        "\n",
        "Modify the implementation so that it is exactly Curl\n",
        "\n",
        "\n",
        "Try to use the **learn_on_batch** function to make the ema update : [here's where you begin](https://github.com/ray-project/ray/blob/ec3243d78726a2840f1323f997a210d1f33e5656/rllib/policy/tf_policy.py)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}